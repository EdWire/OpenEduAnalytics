{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### **Authorship**\n",
        "\n",
        "The inspiration / reference / source of this code was the [Refine_EdFi.ipynb](https://github.com/EdWire/OpenEduAnalytics/blob/feature/saas_deploy/modules/module_catalog/Ed-Fi/notebook/Refine_EdFi.ipynb) notebook  (hereby called \"**Original Referenced Code**\") originally authored by [Abhinav](https://github.com/Abhinavgundapaneni). \n",
        "\n",
        "[Viraj Jayant](https://github.com/virajjayant-neenopal) has made edits and modifications to the Original Referenced Code from the forked repository to tailor it to the add more customizations.\n",
        "\n",
        "##### **Major Changes**\n",
        "\n",
        "Here are the major changes Viraj Jayant made to the Original Referenced Code:\n",
        "\n",
        "1. Leveraging **partitioning** within functions upsert, overwrite, append: these base functions are modified in OEA class for the same\n",
        "2. Extending changes to Original Referenced Code to include ed-fi extensions when _ext column present (**extensions like TPDM, TX, etc.**)\n",
        "3. Changes to `add_to_lake_db` oea base function to include optional overwrite mode and also facilitate references to the ext tables (suffixed as _tx, _tpdm, etc.)\n",
        "4. ETL of specific entities if parameterized set to True (also implemented in Ingestion and Landing Notebooks)\n",
        "5. Making the codebase generally more compliant with OEA than before\n",
        "6. Edit to OEAUtils function `create_spark_schemas_from_definitions` to check if `x-Ed-Fi-explode` is present or not\n",
        "\n",
        "##### **Additional Notes**\n",
        "As an additional point please note that both the Original Referenced Code and this notebook implements partitioning after entity in the directory. Please consider the following example:\n",
        "1. **Original OEA**: stage2/Refined/Ed-Fi/5.2/`DistrictId=All/SchoolYear=All/ed-fi/general/weaponDescriptors`\n",
        "2. **New Directory**: stage2/Refined/Ed-Fi/5.2/`ed-fi/general/weaponDescriptors/DistrictId=All/SchoolYear=All`  \n",
        "\n",
        "##### **Specific Code Reference**\n",
        "\n",
        "Original Referenced Code is referenced from the following location:\n",
        "[Original Code Link](https://github.com/EdWire/OpenEduAnalytics/blob/feature/saas_deploy/modules/module_catalog/Ed-Fi/notebook/Refine_EdFi.ipynb)\n",
        "\n",
        "**Viraj Jayant** has made edits and customizations to this code to suit the project's needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run /edfi_fetch_urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import pyspark.sql.functions as f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "districtPath = districtId if districtId != None else \"All\"\n",
        "schoolYearPath = schoolYear if schoolYear != None else \"All\"\n",
        "\n",
        "parameterized = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%run edfi_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "if parameterized == True:\n",
        "    edfiEntitiesPath = f'stage1/Transactional/{moduleName}/{apiVersion}/DistrictId={districtPath}/SchoolYear={schoolYearPath}/etl_entities/current_run_data'\n",
        "\n",
        "    _, edfiEntities = edfi.listSpecifiedEntities(edfiEntitiesPath)\n",
        "else:\n",
        "    edfiEntities = \"All\"  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# 1) set the workspace (this determines where in the data lake you'll be writing to and reading from).\n",
        "# You can work in 'dev', 'prod', or a sandbox with any name you choose.\n",
        "# For example, Sam the developer can create a 'sam' workspace and expect to find his datasets in the data lake under oea/sandboxes/sam\n",
        "workspace = 'dev'\n",
        "oea.set_workspace(workspace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "schema_gen = OpenAPIUtil(swagger_url)\n",
        "schemas = schema_gen.create_spark_schemas()\n",
        "primitive_datatypes = ['timestamp', 'date', 'decimal', 'boolean', 'integer', 'string', 'long']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def get_descriptor_schema(descriptor):\n",
        "    fields = []\n",
        "    fields.append(StructField('_etag',LongType(), True))\n",
        "    fields.append(StructField(f\"{descriptor[:-1]}Id\", IntegerType(), True))\n",
        "    fields.append(StructField('codeValue',StringType(), True))\n",
        "    fields.append(StructField('description',StringType(), True))\n",
        "    fields.append(StructField('id',StringType(), True))\n",
        "    fields.append(StructField('namespace',StringType(), True))\n",
        "    fields.append(StructField('shortDescription',StringType(), True))\n",
        "    return StructType(fields)\n",
        "\n",
        "def get_descriptor_metadata(descriptor):\n",
        "    return [['_etag', 'long', 'no-op'],\n",
        "            [f\"{descriptor[:-1]}Id\", 'integer', 'hash'],\n",
        "            ['codeValue','string', 'no-op'],\n",
        "            ['description','string', 'no-op'],\n",
        "            ['id','string', 'no-op'],\n",
        "            ['namespace','string', 'no-op'],\n",
        "            ['shortDescription','string', 'no-op']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def has_column(df, col):\n",
        "    try:\n",
        "        df[col]\n",
        "        return True\n",
        "    except AnalysisException:\n",
        "        return False\n",
        "\n",
        "def modify_descriptor_value(df, col_name):\n",
        "    if col_name in df.columns:\n",
        "        # TODO: @Abhinav, I do not see where you made the changes to use the descriptorId instead of Namespace/CodeValue\n",
        "        df = df.withColumn(f\"{col_name}LakeId\", f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.regexp_replace(col_name, '#', '_')))\n",
        "        df = df.drop(col_name)\n",
        "    else:\n",
        "        df = df.withColumn(f\"{col_name}LakeId\", f.lit(None).cast(\"String\"))\n",
        "\n",
        "    return df\n",
        "\n",
        "def flatten_reference_col(df, target_col):\n",
        "    col_prefix = target_col.name.replace('Reference', '')\n",
        "    df = df.withColumn(f\"{col_prefix}LakeId\", f.when(f.col(target_col.name).isNotNull(), f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.split(f.col(f'{target_col.name}.link.href'), '/').getItem(3))))\n",
        "    df = df.drop(target_col.name)\n",
        "    return df\n",
        "\n",
        "def modify_references_and_descriptors(df, target_col):\n",
        "    for ref_col in [x for x in df.columns if re.search('Reference$', x) is not None]:\n",
        "        df = flatten_reference_col(df, target_col.dataType.elementType[ref_col])\n",
        "    for desc_col in [x for x in df.columns if re.search('Descriptor$', x) is not None]:\n",
        "        df = modify_descriptor_value(df, desc_col)\n",
        "    return df\n",
        "\n",
        "def explode_arrays(df, sink_general_path,target_col, schema_name, table_name, extension = None):\n",
        "    cols = ['lakeId', 'DistrictId', 'SchoolYear']\n",
        "    child_df = df.select(cols + [target_col.name])\n",
        "    child_df = child_df.withColumn(\"exploded\", f.explode(target_col.name)).drop(target_col.name).select(cols + ['exploded.*'])\n",
        "\n",
        "    # TODO: It looks like te {target_col.name}LakeId column is not addedd to the child entities\n",
        "    #       We should use LakeId suffix when using the \"id\" column from the parent and HKey suffix when creating a Hash Key based on composite key columns\n",
        "    identity_cols = [x.name for x in target_col.dataType.elementType.fields if 'x-Ed-Fi-isIdentity' in x.metadata].sort()\n",
        "    if(identity_cols is not None and len(identity_cols) > 0):\n",
        "        child_df = child_df.withColumn(f\"{target_col.name}LakeId\", f.concat(f.col('DistrictId'), f.lit('_'), f.col('SchoolYear'), f.lit('_'), *[f.concat(f.col(x), f.lit('_')) for x in identity_cols]))\n",
        "    \n",
        "    # IMPORTANT: We must modify Reference and Descriptor columns for child columns \"first\". \n",
        "    # This must be done \"after\" the composite key from identity_cols has been created otherwise the columns are renamed and will not be found by identity_cols.\n",
        "    # This must be done \"before\" the grand_child is exploded below\n",
        "    child_df = modify_references_and_descriptors(child_df, target_col)\n",
        "\n",
        "    for array_sub_col in [x for x in target_col.dataType.elementType.fields if x.dataType.typeName() == 'array' ]:\n",
        "        grand_child_df = child_df.withColumn('exploded', f.explode(array_sub_col.name)).select(child_df.columns + ['exploded.*']).drop(array_sub_col.name)\n",
        "        \n",
        "        # Modifying Reference and Descriptor columns for the grand_child array\n",
        "        grand_child_df = modify_references_and_descriptors(grand_child_df, array_sub_col)\n",
        "\n",
        "        logger.info(f\"Writing Grand Child Table - {table_name}_{target_col.name}_{array_sub_col.name}\")\n",
        "        oea.upsert(df = grand_child_df, \n",
        "                   destination_path = f\"{sink_general_path}_{target_col.name}_{array_sub_col.name}\", \n",
        "                   primary_key = 'lakeId',\n",
        "                   partitioning = True,\n",
        "                   partitioning_cols = ['DistrictId', 'SchoolYear']) \n",
        "        oea.add_to_lake_db(source_entity_path = f\"{sink_general_path}_{target_col.name}_{array_sub_col.name}\", \n",
        "                           overwrite = True,\n",
        "                           extension = extension)\n",
        "        #grand_child_df.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').save(oea.to_url(f\"{sink_general_path}_{target_col.name}_{array_sub_col.name}\"))\n",
        "\n",
        "    logger.info(f\"Writing Child Table - {table_name}_{target_col.name}\")\n",
        "    oea.upsert(df = child_df, \n",
        "               destination_path = f\"{sink_general_path}_{target_col.name}\", \n",
        "               primary_key = 'lakeId',\n",
        "               partitioning = True,\n",
        "               partitioning_cols = ['DistrictId', 'SchoolYear']) \n",
        "    oea.add_to_lake_db(source_entity_path = f\"{sink_general_path}_{target_col.name}\",\n",
        "                       overwrite = True,\n",
        "                       extension = extension)\n",
        "    #child_df.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').save(oea.to_url(f\"{sink_general_path}_{target_col.name}\"))\n",
        "\n",
        "    # Drop array column from parent entity\n",
        "    df = df.drop(target_col.name)\n",
        "    return df\n",
        "\n",
        "def transform(df, \n",
        "              schema_name, \n",
        "              table_name, \n",
        "              primary_key,\n",
        "              ext_entity,\n",
        "              sink_general_path,\n",
        "              parent_schema_name, \n",
        "              parent_table_name):\n",
        "    if re.search('Descriptors$', table_name) is None:\n",
        "        # Use Deep Copy otherwise the schemas object also gets modified every time target_schema is modified\n",
        "        target_schema = copy.deepcopy(schemas[table_name])\n",
        "        # Add primary key\n",
        "        if has_column(df, primary_key):\n",
        "            df = df.withColumn('lakeId', f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.col(primary_key)).cast(\"String\"))\n",
        "        else:\n",
        "            df = df.withColumn('lakeId', f.lit(None).cast(\"String\"))\n",
        "    else:\n",
        "        target_schema = get_descriptor_schema(table_name)\n",
        "        # Add primary key\n",
        "        if has_column(df, 'codeValue') and has_column(df, 'namespace'):\n",
        "            # TODO: @Abhinav, I do not see where you made the changes to use the descriptorId instead of Namespace/CodeValue\n",
        "            df = df.withColumn('lakeId', f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.col('namespace'), f.col('codeValue')).cast(\"String\"))\n",
        "        else:\n",
        "            df = df.withColumn('lakeId', f.lit(None).cast(\"String\"))\n",
        "\n",
        "    target_schema = target_schema.add(StructField('DistrictId', StringType()))\\\n",
        "                                 .add(StructField('SchoolYear', StringType()))\\\n",
        "                                 .add(StructField('LastModifiedDate', TimestampType()))\n",
        "\n",
        "    df = transform_sub_module(df, target_schema, sink_general_path, schema_name, table_name)\n",
        "    logger.info(f\"Writing Main Table - {table_name}\")\n",
        "    oea.upsert(df = df, \n",
        "               destination_path = f\"{sink_general_path}\", \n",
        "               primary_key = 'lakeId',\n",
        "               partitioning = True,\n",
        "               partitioning_cols = ['DistrictId', 'SchoolYear']) \n",
        "    oea.add_to_lake_db(source_entity_path = sink_general_path, \n",
        "                       overwrite = True,\n",
        "                       extension = None)\n",
        "\n",
        "    if '_ext' in df.columns:\n",
        "        target_schema = get_ext_entities_schemas(table_name = table_name,\n",
        "                                                 ext_column_name = '_ext',\n",
        "                                                 default_value = ext_entity)\n",
        "        df = flatten_ext_column(df = df, \n",
        "                                table_name = table_name, \n",
        "                                ext_col = '_ext', \n",
        "                                inner_key = ext_entity,\n",
        "                                ext_inner_cols = target_schema.fieldNames())\n",
        "        sink_general_path = sink_general_path.replace('/ed-fi/', f'/{ext_entity.lower()}/')\n",
        "        df = transform_sub_module(df, \n",
        "                                  target_schema, \n",
        "                                  sink_general_path, \n",
        "                                  schema_name,\n",
        "                                  table_name,\n",
        "                                  extension = f\"_{ext_entity.lower()}\")\n",
        "\n",
        "        logger.info(f\"Writing EXT Table - {table_name}\")\n",
        "        oea.upsert(df = df, \n",
        "                   destination_path = f\"{sink_general_path}\", \n",
        "                   primary_key = 'lakeId',\n",
        "                   partitioning = True,\n",
        "                   partitioning_cols = ['DistrictId', 'SchoolYear']) \n",
        "        oea.add_to_lake_db(sink_general_path, \n",
        "                           overwrite = True,\n",
        "                           extension = f\"_{ext_entity.lower()}\")\n",
        "        \n",
        "def transform_sub_module(df, target_schema, sink_general_path, schema_name, table_name, extension = None):\n",
        "    for col_name in target_schema.fieldNames():\n",
        "        target_col = target_schema[col_name]\n",
        "        # If Primitive datatype, i.e String, Bool, Integer, etc.abs\n",
        "        # Note: Descriptor is a String therefore is a Primitive datatype\n",
        "        if target_col.dataType.typeName() in primitive_datatypes:\n",
        "            # If it is a Descriptor\n",
        "            if re.search('Descriptor$', col_name) is not None:\n",
        "                df = modify_descriptor_value(df, col_name)\n",
        "            else:\n",
        "                if col_name in df.columns:\n",
        "                    # Casting columns to primitive data types\n",
        "                    df = df.withColumn(col_name, f.col(col_name).cast(target_col.dataType))\n",
        "                else:\n",
        "                    # If Column not present in dataframe, add column with None values.\n",
        "                    df = df.withColumn(col_name, f.lit(None).cast(target_col.dataType))\n",
        "        # If Complex datatype, i.e. Object, Array\n",
        "        else:\n",
        "            if col_name not in df.columns:\n",
        "                df = df.withColumn(col_name, f.lit(None).cast(target_col.dataType))\n",
        "            else:\n",
        "                # Generate JSON column as a Complex Type\n",
        "                df = df.withColumn(f\"{col_name}_json\", f.to_json(f.col(col_name))) \\\n",
        "                    .withColumn(col_name, f.from_json(f.col(f\"{col_name}_json\"), target_col.dataType)) \\\n",
        "                    .drop(f\"{col_name}_json\")\n",
        "            \n",
        "            # Modify the links with surrogate keys\n",
        "            if re.search('Reference$', col_name) is not None:\n",
        "                df = flatten_reference_col(df, target_col)\n",
        "    \n",
        "            if target_col.dataType.typeName() == 'array':\n",
        "                df = explode_arrays(df, sink_general_path,target_col, schema_name, table_name, extension = extension)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def get_ext_entities_schemas(table_name = 'staffs',\n",
        "                             ext_column_name = '_ext',\n",
        "                             default_value = 'TPDM'):\n",
        "    target_schema = copy.deepcopy(schemas[table_name])\n",
        "    for col_name in target_schema.fieldNames():\n",
        "        target_col = target_schema[col_name]\n",
        "        if target_col.name == ext_column_name:\n",
        "            if target_col.dataType[0].name == default_value:\n",
        "                return target_col.dataType[0].dataType         \n",
        "                \n",
        "def flatten_ext_column(df, \n",
        "                       table_name, \n",
        "                       ext_col, \n",
        "                       inner_key,\n",
        "                       ext_inner_cols\n",
        "                       ):\n",
        "    cols = ['lakeId', 'DistrictId', 'SchoolYear', 'id_pseudonym']\n",
        "    flattened_cols = ext_inner_cols#[\"educatorPreparationPrograms\"] #_ext_TX_cols[table_name]\n",
        "    dict_col = F.col(ext_col)[inner_key]\n",
        "    complex_dtype_text = str(df.select('_ext').dtypes[0][1])\n",
        "\n",
        "    exprs = [dict_col.getItem(key).alias(key) for key in flattened_cols if str(key) in complex_dtype_text]\n",
        "    flattened_df = df.select(exprs + cols)\n",
        "    return flattened_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def sink_path_cleanup(destination_path):\n",
        "    pattern = re.compile(r'DistrictId=.*?/|SchoolYear=.*?/')\n",
        "    destination_path = re.sub(pattern, '', destination_path)\n",
        "\n",
        "    return destination_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def refine_and_explode_data(schema_name, \n",
        "                            tables_source,\n",
        "                            ext_entity,\n",
        "                            metadata, \n",
        "                            transform_mode, \n",
        "                            test_mode,\n",
        "                            items = []):\n",
        "    global districtPath,schoolYearPath\n",
        "    if items == 'All':\n",
        "        items = oea.get_folders(f\"stage2/Ingested/{tables_source}\")\n",
        "    for item in items:\n",
        "            table_name = item #sap_to_edfi_complex[item]\n",
        "            table_path = f\"{tables_source}/{item}\"\n",
        "            if not(oea.path_exists(f\"stage2/Ingested/{table_path}\")):\n",
        "                print(table_path)\n",
        "                continue            \n",
        "            logger.info(f\"Processing schema/table: {schema_name}/{table_name}\")\n",
        "            if item == 'metadata.csv':\n",
        "                logger.info('ignore metadata processing, since this is not a table to be ingested')\n",
        "            else:\n",
        "                try:\n",
        "                    if not(transform_mode):\n",
        "                        df = oea.refine(table_path, \n",
        "                                        metadata = metadata[item], \n",
        "                                        primary_key = 'id')\n",
        "                    if transform_mode:\n",
        "                        logger.info('Ed-Fi to Ed-Fi Relationship Model: ' + table_name)               \n",
        "                        source_path = f'stage2/Ingested/{table_path}'\n",
        "                        sink_general_path, sink_sensitive_path = oea.get_sink_general_sensitive_paths(source_path)\n",
        "                        \n",
        "                        sink_general_path = sink_path_cleanup(sink_general_path)\n",
        "                        sink_sensitive_path = sink_path_cleanup(sink_sensitive_path)\n",
        "\n",
        "                        df_changes = oea.get_latest_changes(source_path, sink_general_path)\n",
        "                        df_changes = df_changes.withColumn('DistrictId', F.lit(districtPath))\n",
        "                        df_changes = df_changes.withColumn('SchoolYear', F.lit(schoolYearPath))\n",
        "                        \n",
        "                        if df_changes.count() > 0:\n",
        "                            df_pseudo, df_lookup = oea.pseudonymize(df_changes, \n",
        "                                                                    metadata,\n",
        "                                                                    transform_mode)\n",
        "                            \n",
        "                            transform(df_pseudo, \n",
        "                                      schema_name, \n",
        "                                      table_name, \n",
        "                                      'id_pseudonym', \n",
        "                                      ext_entity, \n",
        "                                      sink_general_path,\n",
        "                                      None, \n",
        "                                      None)\n",
        "                            \n",
        "                            oea.upsert(df = df_lookup, \n",
        "                                       destination_path = sink_sensitive_path, \n",
        "                                       primary_key = 'id',\n",
        "                                       partitioning = True,\n",
        "                                       partitioning_cols = ['DistrictId', 'SchoolYear'])    \n",
        "                            oea.add_to_lake_db(source_entity_path = sink_sensitive_path,\n",
        "                                               overwrite = True,\n",
        "                                               extension = None)\n",
        "                        else:\n",
        "                            logger.info(f'No updated rows in {source_path} to process.')\n",
        "\n",
        "                except AnalysisException as e:\n",
        "                    # This means the table may have not been properly refined due to errors with the primary key not aligning with columns expected in the lookup table.\n",
        "                    logger.info(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "schema_name = 'ed-fi'\n",
        "ext_entity = 'TPDM'\n",
        "test_mode = False\n",
        "transform_mode = True\n",
        "tables_source = f'{moduleName}/{apiVersion}/DistrictId={districtPath}/SchoolYear={schoolYearPath}/{schema_name}'\n",
        "transform_items = edfiEntities \n",
        "metadata = oea.get_metadata_from_url(metadataUrl)\n",
        "\n",
        "refine_and_explode_data(schema_name, \n",
        "                        tables_source,\n",
        "                        ext_entity,\n",
        "                        metadata,\n",
        "                        transform_mode, \n",
        "                        test_mode,\n",
        "                        items = transform_items)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
