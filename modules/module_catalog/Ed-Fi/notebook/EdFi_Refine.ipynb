{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### **Authorship**\n",
        "\n",
        "The inspiration / reference / source of this code was the [Refine_EdFi.ipynb](https://github.com/EdWire/OpenEduAnalytics/blob/feature/saas_deploy/modules/module_catalog/Ed-Fi/notebook/Refine_EdFi.ipynb) notebook  (hereby called \"**Original Referenced Code**\") originally authored by [Abhinav](https://github.com/Abhinavgundapaneni). \n",
        "\n",
        "[Viraj Jayant](https://github.com/virajjayant-neenopal) has made edits and modifications to the Original Referenced Code from the forked repository to tailor it to the add more customizations.\n",
        "\n",
        "##### **Major Changes**\n",
        "\n",
        "Here are the major changes Viraj Jayant made to the Original Referenced Code:\n",
        "\n",
        "1. Leveraging **partitioning** within functions upsert, overwrite, append: these base functions are modified in OEA class for the same\n",
        "2. Extending changes to Original Referenced Code to include ed-fi extensions when _ext column present (**extensions like TPDM, TX, etc.**)\n",
        "3. Changes to `add_to_lake_db` oea base function to include optional overwrite mode and also facilitate references to the ext tables (suffixed as _tx, _tpdm, etc.)\n",
        "4. ETL of specific entities if parameterized set to True (also implemented in Ingestion and Landing Notebooks)\n",
        "5. Making the codebase generally more compliant with OEA than before\n",
        "6. Edit to OEAUtils function `create_spark_schemas_from_definitions` to check if `x-Ed-Fi-explode` is present or not\n",
        "\n",
        "##### **Additional Notes**\n",
        "As an additional point please note that both the Original Referenced Code and this notebook implements partitioning after entity in the directory. Please consider the following example:\n",
        "1. **Original OEA**: stage2/Refined/Ed-Fi/5.2/`DistrictId=All/SchoolYear=All/ed-fi/general/weaponDescriptors`\n",
        "2. **New Directory**: stage2/Refined/Ed-Fi/5.2/`ed-fi/general/weaponDescriptors/DistrictId=All/SchoolYear=All`  \n",
        "\n",
        "##### **Specific Code Reference**\n",
        "\n",
        "Original Referenced Code is referenced from the following location:\n",
        "[Original Code Link](https://github.com/EdWire/OpenEduAnalytics/blob/feature/saas_deploy/modules/module_catalog/Ed-Fi/notebook/Refine_EdFi.ipynb)\n",
        "\n",
        "**Viraj Jayant** has made edits and customizations to this code to suit the project's needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-10-20T09:19:47.5876201Z",
              "execution_start_time": "2023-10-20T09:19:47.5873139Z",
              "livy_statement_state": "available",
              "parent_msg_id": "84a8525b-137f-4b91-a2d3-2bac93602c0a",
              "queued_time": "2023-10-20T09:19:45.9426302Z",
              "session_id": "32",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": null,
              "state": "finished",
              "statement_id": -1
            },
            "text/plain": [
              "StatementMeta(, 32, -1, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%run /edfi_fetch_urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-10-20T09:19:49.2197441Z",
              "execution_start_time": "2023-10-20T09:19:49.0228109Z",
              "livy_statement_state": "available",
              "parent_msg_id": "98e73440-91f2-4e74-93d0-52d0b4217dd9",
              "queued_time": "2023-10-20T09:19:48.8227167Z",
              "session_id": "32",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "spark3p3sm",
              "state": "finished",
              "statement_id": 49
            },
            "text/plain": [
              "StatementMeta(spark3p3sm, 32, 49, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import copy\n",
        "import pyspark.sql.functions as f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-10-20T09:19:49.5770555Z",
              "execution_start_time": "2023-10-20T09:19:49.402834Z",
              "livy_statement_state": "available",
              "parent_msg_id": "d07ba542-1309-4b33-aeb3-a67372844677",
              "queued_time": "2023-10-20T09:19:49.0165914Z",
              "session_id": "32",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "spark3p3sm",
              "state": "finished",
              "statement_id": 50
            },
            "text/plain": [
              "StatementMeta(spark3p3sm, 32, 50, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "districtPath = districtId if districtId != None else \"All\"\n",
        "schoolYearPath = schoolYear if schoolYear != None else \"All\"\n",
        "swagger_url = swaggerUrl\n",
        "\n",
        "parameterized = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-10-20T09:19:54.1847802Z",
              "execution_start_time": "2023-10-20T09:19:54.184507Z",
              "livy_statement_state": "available",
              "parent_msg_id": "c24726b4-345f-48e8-93a1-1254df24c8d9",
              "queued_time": "2023-10-20T09:19:50.1204469Z",
              "session_id": "32",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": null,
              "state": "finished",
              "statement_id": -1
            },
            "text/plain": [
              "StatementMeta(, 32, -1, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-10-20 09:19:51,599 - OEA - INFO - Now using workspace: dev\n",
            "2023-10-20 09:19:51,600 - OEA - INFO - OEA initialized.\n",
            "2023-10-20 09:19:52,279 - OEA - INFO - Now using workspace: dev\n",
            "2023-10-20 09:19:53,446 - OEA - INFO - Now using workspace: dev\n",
            "2023-10-20 09:19:53,447 - OEA - INFO - OEA initialized.\n",
            "2023-10-20 09:19:53,447 - OEA - INFO - minChangeVersion=None and maxChangeVersion=None\n",
            "2023-10-20 09:19:53,607 - OEA - INFO - failed to retrieve clientId and clientSecret from keyvault with exception: An error occurred while calling z:com.microsoft.azure.synapse.tokenlibrary.TokenLibrary.getSecret.\n",
            ": com.microsoft.azure.synapse.tokenlibrary.utils.AkvUtils$KeyVaultException: Azure Key Vault returned error code 'SecretNotFound' with message 'A secret with (name/id) edfi-clientid was not found in this key vault. If you recently deleted this secret you may be able to recover it using the correct recovery command. For help resolving this issue, please see https://go.microsoft.com/fwlink/?linkid=2125182' and inner error 'None'\n",
            "\tat com.microsoft.azure.synapse.tokenlibrary.utils.AkvUtils$KeyVaultErrorResponse.toException(AkvUtils.scala:71)\n",
            "\tat com.microsoft.azure.synapse.tokenlibrary.utils.AkvUtils$.$anonfun$performAkvRequest$3(AkvUtils.scala:97)\n",
            "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
            "\tat scala.util.Success.map(Try.scala:213)\n",
            "\tat com.microsoft.azure.synapse.tokenlibrary.utils.AkvUtils$.$anonfun$performAkvRequest$1(AkvUtils.scala:97)\n",
            "\tat com.twitter.util.Future.$anonfun$flatMap$1(Future.scala:1808)\n",
            "\tat com.twitter.util.Promise$FutureTransformer.liftedTree1$1(Promise.scala:240)\n",
            "\tat com.twitter.util.Promise$FutureTransformer.k(Promise.scala:240)\n",
            "\tat com.twitter.util.Promise$Transformer.apply(Promise.scala:215)\n",
            "\tat com.twitter.util.Promise$WaitQueue.com$twitter$util$Promise$WaitQueue$$run(Promise.scala:91)\n",
            "\tat com.twitter.util.Promise$WaitQueue$$anon$1.run(Promise.scala:86)\n",
            "\tat com.twitter.concurrent.LocalScheduler$Activation.run(Scheduler.scala:198)\n",
            "\tat com.twitter.concurrent.LocalScheduler$Activation.submit(Scheduler.scala:157)\n",
            "\tat com.twitter.concurrent.LocalScheduler.submit(Scheduler.scala:274)\n",
            "\tat com.twitter.concurrent.Scheduler$.submit(Scheduler.scala:109)\n",
            "\tat com.twitter.util.Promise$WaitQueue.runInScheduler(Promise.scala:86)\n",
            "\tat com.twitter.util.Promise.updateIfEmpty(Promise.scala:778)\n",
            "\tat com.twitter.util.Promise.update(Promise.scala:750)\n",
            "\tat com.twitter.util.Promise.setValue(Promise.scala:726)\n",
            "\tat com.twitter.concurrent.AsyncQueue.offer(AsyncQueue.scala:123)\n",
            "\tat com.twitter.finagle.netty4.transport.ChannelTransport$$anon$2.channelRead(ChannelTransport.scala:168)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n",
            "\tat com.twitter.finagle.netty4.http.handler.UnpoolHttpHandler$.channelRead(UnpoolHttpHandler.scala:32)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n",
            "\tat com.twitter.finagle.netty4.http.handler.ClientExceptionMapper$.channelRead(ClientExceptionMapper.scala:35)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n",
            "\tat com.twitter.finagle.netty4.http.handler.HeaderValidatorHandler$.channelRead(HeaderValidatorHandler.scala:51)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n",
            "\tat shadenetty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n",
            "\tat shadenetty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n",
            "\tat shadenetty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:438)\n",
            "\tat shadenetty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:323)\n",
            "\tat shadenetty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:297)\n",
            "\tat shadenetty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:253)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n",
            "\tat shadenetty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n",
            "\tat shadenetty.handler.ssl.SslHandler.unwrap(SslHandler.java:1432)\n",
            "\tat shadenetty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1199)\n",
            "\tat shadenetty.handler.ssl.SslHandler.decode(SslHandler.java:1243)\n",
            "\tat shadenetty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:502)\n",
            "\tat shadenetty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:441)\n",
            "\tat shadenetty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:278)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n",
            "\tat shadenetty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1434)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n",
            "\tat shadenetty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n",
            "\tat shadenetty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:965)\n",
            "\tat shadenetty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n",
            "\tat shadenetty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:648)\n",
            "\tat shadenetty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:583)\n",
            "\tat shadenetty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:500)\n",
            "\tat shadenetty.channel.nio.NioEventLoop.run(NioEventLoop.java:462)\n",
            "\tat shadenetty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat com.twitter.finagle.util.BlockingTimeTrackingThreadFactory$$anon$1.run(BlockingTimeTrackingThreadFactory.scala:23)\n",
            "\tat shadenetty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "\n",
            "2023-10-20 09:19:53,607 - OEA - INFO - defaulting to test data\n"
          ]
        }
      ],
      "source": [
        "%run edfi_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-10-20T09:19:54.6297426Z",
              "execution_start_time": "2023-10-20T09:19:54.4170896Z",
              "livy_statement_state": "available",
              "parent_msg_id": "a07acd70-cae2-41a4-a501-fe4ae1e6ace4",
              "queued_time": "2023-10-20T09:19:50.6236219Z",
              "session_id": "32",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "spark3p3sm",
              "state": "finished",
              "statement_id": 58
            },
            "text/plain": [
              "StatementMeta(spark3p3sm, 32, 58, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if parameterized == True:\n",
        "    edfiEntitiesPath = f'stage1/Transactional/{moduleName}/{apiVersion}/DistrictId={districtPath}/SchoolYear={schoolYearPath}/etl_entities/current_run_data'\n",
        "\n",
        "    _, edfiEntities = edfi.listSpecifiedEntities(edfiEntitiesPath)\n",
        "else:\n",
        "    edfiEntities = \"All\"  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-10-20T09:19:55.0164843Z",
              "execution_start_time": "2023-10-20T09:19:54.8322366Z",
              "livy_statement_state": "available",
              "parent_msg_id": "26d13c79-106b-4d82-aabc-15cc440c96dc",
              "queued_time": "2023-10-20T09:19:51.1402798Z",
              "session_id": "32",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "spark3p3sm",
              "state": "finished",
              "statement_id": 59
            },
            "text/plain": [
              "StatementMeta(spark3p3sm, 32, 59, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-10-20 09:19:54,816 - OEA - INFO - Now using workspace: dev\n"
          ]
        }
      ],
      "source": [
        "# 1) set the workspace (this determines where in the data lake you'll be writing to and reading from).\n",
        "# You can work in 'dev', 'prod', or a sandbox with any name you choose.\n",
        "# For example, Sam the developer can create a 'sam' workspace and expect to find his datasets in the data lake under oea/sandboxes/sam\n",
        "workspace = 'dev'\n",
        "oea.set_workspace(workspace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-10-20T09:19:56.3853456Z",
              "execution_start_time": "2023-10-20T09:19:55.2244875Z",
              "livy_statement_state": "available",
              "parent_msg_id": "1ab3f273-4115-41ae-b0fd-31632fa8a445",
              "queued_time": "2023-10-20T09:19:51.9519696Z",
              "session_id": "32",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "spark3p3sm",
              "state": "finished",
              "statement_id": 60
            },
            "text/plain": [
              "StatementMeta(spark3p3sm, 32, 60, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "localEducationAgencyReference\n"
          ]
        }
      ],
      "source": [
        "schema_gen = OpenAPIUtil(swagger_url)\n",
        "schemas = schema_gen.create_spark_schemas()\n",
        "primitive_datatypes = ['timestamp', 'date', 'decimal', 'boolean', 'integer', 'string', 'long']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-10-20T09:19:56.7679747Z",
              "execution_start_time": "2023-10-20T09:19:56.5576252Z",
              "livy_statement_state": "available",
              "parent_msg_id": "39e7360a-3f11-4f29-b165-e7b2be919f7c",
              "queued_time": "2023-10-20T09:19:52.4216835Z",
              "session_id": "32",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "spark3p3sm",
              "state": "finished",
              "statement_id": 61
            },
            "text/plain": [
              "StatementMeta(spark3p3sm, 32, 61, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def get_descriptor_schema(descriptor):\n",
        "    fields = []\n",
        "    fields.append(StructField('_etag',LongType(), True))\n",
        "    fields.append(StructField(f\"{descriptor[:-1]}Id\", IntegerType(), True))\n",
        "    fields.append(StructField('codeValue',StringType(), True))\n",
        "    fields.append(StructField('description',StringType(), True))\n",
        "    fields.append(StructField('id',StringType(), True))\n",
        "    fields.append(StructField('namespace',StringType(), True))\n",
        "    fields.append(StructField('shortDescription',StringType(), True))\n",
        "    return StructType(fields)\n",
        "\n",
        "def get_descriptor_metadata(descriptor):\n",
        "    return [['_etag', 'long', 'no-op'],\n",
        "            [f\"{descriptor[:-1]}Id\", 'integer', 'hash'],\n",
        "            ['codeValue','string', 'no-op'],\n",
        "            ['description','string', 'no-op'],\n",
        "            ['id','string', 'no-op'],\n",
        "            ['namespace','string', 'no-op'],\n",
        "            ['shortDescription','string', 'no-op']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-10-20T09:19:57.1456146Z",
              "execution_start_time": "2023-10-20T09:19:56.9615351Z",
              "livy_statement_state": "available",
              "parent_msg_id": "109ec3db-dd0d-4cd1-9d6f-a48531d48c1d",
              "queued_time": "2023-10-20T09:19:52.8858857Z",
              "session_id": "32",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "spark3p3sm",
              "state": "finished",
              "statement_id": 62
            },
            "text/plain": [
              "StatementMeta(spark3p3sm, 32, 62, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def has_column(df, col):\n",
        "    try:\n",
        "        df[col]\n",
        "        return True\n",
        "    except AnalysisException:\n",
        "        return False\n",
        "\n",
        "def modify_descriptor_value(df, col_name):\n",
        "    if col_name in df.columns:\n",
        "        # TODO: @Abhinav, I do not see where you made the changes to use the descriptorId instead of Namespace/CodeValue\n",
        "        df = df.withColumn(f\"{col_name}LakeId\", f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.regexp_replace(col_name, '#', '_')))\n",
        "        df = df.drop(col_name)\n",
        "    else:\n",
        "        df = df.withColumn(f\"{col_name}LakeId\", f.lit(None).cast(\"String\"))\n",
        "\n",
        "    return df\n",
        "\n",
        "def flatten_reference_col(df, target_col):\n",
        "    col_prefix = target_col.name.replace('Reference', '')\n",
        "    df = df.withColumn(f\"{col_prefix}LakeId\", f.when(f.col(target_col.name).isNotNull(), f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.split(f.col(f'{target_col.name}.link.href'), '/').getItem(3))))\n",
        "    df = df.drop(target_col.name)\n",
        "    return df\n",
        "\n",
        "def modify_references_and_descriptors(df, target_col):\n",
        "    for ref_col in [x for x in df.columns if re.search('Reference$', x) is not None]:\n",
        "        df = flatten_reference_col(df, target_col.dataType.elementType[ref_col])\n",
        "    for desc_col in [x for x in df.columns if re.search('Descriptor$', x) is not None]:\n",
        "        df = modify_descriptor_value(df, desc_col)\n",
        "    return df\n",
        "\n",
        "def explode_arrays(df, sink_general_path,target_col, schema_name, table_name, extension = None):\n",
        "    # TODO: Assess if LastModifiedDate inclusion breaks ETL or not\n",
        "    try:\n",
        "        cols = ['lakeId', 'DistrictId', 'SchoolYear', 'LastModifiedDate']\n",
        "        child_df = df.select(cols + [target_col.name])\n",
        "    except:\n",
        "        cols = ['lakeId', 'DistrictId', 'SchoolYear']\n",
        "        child_df = df.select(cols + [target_col.name])\n",
        "    child_df = child_df.withColumn(\"exploded\", f.explode(target_col.name)).drop(target_col.name).select(cols + ['exploded.*'])\n",
        "\n",
        "    # TODO: It looks like te {target_col.name}LakeId column is not addedd to the child entities\n",
        "    #       We should use LakeId suffix when using the \"id\" column from the parent and HKey suffix when creating a Hash Key based on composite key columns\n",
        "    identity_cols = [x.name for x in target_col.dataType.elementType.fields if 'x-Ed-Fi-isIdentity' in x.metadata].sort()\n",
        "    if(identity_cols is not None and len(identity_cols) > 0):\n",
        "        child_df = child_df.withColumn(f\"{target_col.name}LakeId\", f.concat(f.col('DistrictId'), f.lit('_'), f.col('SchoolYear'), f.lit('_'), *[f.concat(f.col(x), f.lit('_')) for x in identity_cols]))\n",
        "    \n",
        "    # IMPORTANT: We must modify Reference and Descriptor columns for child columns \"first\". \n",
        "    # This must be done \"after\" the composite key from identity_cols has been created otherwise the columns are renamed and will not be found by identity_cols.\n",
        "    # This must be done \"before\" the grand_child is exploded below\n",
        "    child_df = modify_references_and_descriptors(child_df, target_col)\n",
        "\n",
        "    for array_sub_col in [x for x in target_col.dataType.elementType.fields if x.dataType.typeName() == 'array' ]:\n",
        "        grand_child_df = child_df.withColumn('exploded', f.explode(array_sub_col.name)).select(child_df.columns + ['exploded.*']).drop(array_sub_col.name)\n",
        "        \n",
        "        # Modifying Reference and Descriptor columns for the grand_child array\n",
        "        grand_child_df = modify_references_and_descriptors(grand_child_df, array_sub_col)\n",
        "\n",
        "        logger.info(f\"Writing Grand Child Table - {table_name}_{target_col.name}_{array_sub_col.name}\")\n",
        "        oea.upsert(df = grand_child_df, \n",
        "                   destination_path = f\"{sink_general_path}_{target_col.name}_{array_sub_col.name}\", \n",
        "                   primary_key = 'lakeId',\n",
        "                   partitioning = True,\n",
        "                   partitioning_cols = ['DistrictId', 'SchoolYear']) \n",
        "        oea.add_to_lake_db(source_entity_path = f\"{sink_general_path}_{target_col.name}_{array_sub_col.name}\", \n",
        "                           overwrite = True,\n",
        "                           extension = extension)\n",
        "        #grand_child_df.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').save(oea.to_url(f\"{sink_general_path}_{target_col.name}_{array_sub_col.name}\"))\n",
        "\n",
        "    logger.info(f\"Writing Child Table - {table_name}_{target_col.name}\")\n",
        "    oea.upsert(df = child_df, \n",
        "               destination_path = f\"{sink_general_path}_{target_col.name}\", \n",
        "               primary_key = 'lakeId',\n",
        "               partitioning = True,\n",
        "               partitioning_cols = ['DistrictId', 'SchoolYear']) \n",
        "    oea.add_to_lake_db(source_entity_path = f\"{sink_general_path}_{target_col.name}\",\n",
        "                       overwrite = True,\n",
        "                       extension = extension)\n",
        "    #child_df.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').save(oea.to_url(f\"{sink_general_path}_{target_col.name}\"))\n",
        "\n",
        "    # Drop array column from parent entity\n",
        "    df = df.drop(target_col.name)\n",
        "    return df\n",
        "\n",
        "def transform(df, \n",
        "              schema_name, \n",
        "              table_name, \n",
        "              primary_key,\n",
        "              ext_entity,\n",
        "              sink_general_path,\n",
        "              parent_schema_name, \n",
        "              parent_table_name):\n",
        "    if re.search('Descriptors$', table_name) is None:\n",
        "        # Use Deep Copy otherwise the schemas object also gets modified every time target_schema is modified\n",
        "        target_schema = copy.deepcopy(schemas[table_name])\n",
        "        # Add primary key\n",
        "        if has_column(df, primary_key):\n",
        "            df = df.withColumn('lakeId', f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.col(primary_key)).cast(\"String\"))\n",
        "        else:\n",
        "            df = df.withColumn('lakeId', f.lit(None).cast(\"String\"))\n",
        "    else:\n",
        "        target_schema = get_descriptor_schema(table_name)\n",
        "        # Add primary key\n",
        "        if has_column(df, 'codeValue') and has_column(df, 'namespace'):\n",
        "            # TODO: @Abhinav, I do not see where you made the changes to use the descriptorId instead of Namespace/CodeValue\n",
        "            df = df.withColumn('lakeId', f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.col('namespace'), f.col('codeValue')).cast(\"String\"))\n",
        "        else:\n",
        "            df = df.withColumn('lakeId', f.lit(None).cast(\"String\"))\n",
        "\n",
        "    target_schema = target_schema.add(StructField('DistrictId', StringType()))\\\n",
        "                                 .add(StructField('SchoolYear', StringType()))\\\n",
        "                                 .add(StructField('LastModifiedDate', TimestampType()))\n",
        "\n",
        "    df = transform_sub_module(df, target_schema, sink_general_path, schema_name, table_name)\n",
        "    logger.info(f\"Writing Main Table - {table_name}\")\n",
        "    oea.upsert(df = df, \n",
        "               destination_path = f\"{sink_general_path}\", \n",
        "               primary_key = 'lakeId',\n",
        "               partitioning = True,\n",
        "               partitioning_cols = ['DistrictId', 'SchoolYear']) \n",
        "    oea.add_to_lake_db(source_entity_path = sink_general_path, \n",
        "                       overwrite = True,\n",
        "                       extension = None)\n",
        "\n",
        "    if '_ext' in df.columns:\n",
        "        target_schema = get_ext_entities_schemas(table_name = table_name,\n",
        "                                                 ext_column_name = '_ext',\n",
        "                                                 default_value = ext_entity)\n",
        "        df = flatten_ext_column(df = df, \n",
        "                                table_name = table_name, \n",
        "                                ext_col = '_ext', \n",
        "                                inner_key = ext_entity,\n",
        "                                ext_inner_cols = target_schema.fieldNames())\n",
        "        sink_general_path = sink_general_path.replace('/ed-fi/', f'/{ext_entity.lower()}/')\n",
        "        df = transform_sub_module(df, \n",
        "                                  target_schema, \n",
        "                                  sink_general_path, \n",
        "                                  schema_name,\n",
        "                                  table_name,\n",
        "                                  extension = f\"_{ext_entity.lower()}\")\n",
        "\n",
        "        logger.info(f\"Writing EXT Table - {table_name}\")\n",
        "        oea.upsert(df = df, \n",
        "                   destination_path = f\"{sink_general_path}\", \n",
        "                   primary_key = 'lakeId',\n",
        "                   partitioning = True,\n",
        "                   partitioning_cols = ['DistrictId', 'SchoolYear']) \n",
        "        oea.add_to_lake_db(sink_general_path, \n",
        "                           overwrite = True,\n",
        "                           extension = f\"_{ext_entity.lower()}\")\n",
        "        \n",
        "def transform_sub_module(df, target_schema, sink_general_path, schema_name, table_name, extension = None):\n",
        "    for col_name in target_schema.fieldNames():\n",
        "        target_col = target_schema[col_name]\n",
        "        # If Primitive datatype, i.e String, Bool, Integer, etc.abs\n",
        "        # Note: Descriptor is a String therefore is a Primitive datatype\n",
        "        if target_col.dataType.typeName() in primitive_datatypes:\n",
        "            # If it is a Descriptor\n",
        "            if re.search('Descriptor$', col_name) is not None:\n",
        "                df = modify_descriptor_value(df, col_name)\n",
        "            else:\n",
        "                if col_name in df.columns:\n",
        "                    # Casting columns to primitive data types\n",
        "                    df = df.withColumn(col_name, f.col(col_name).cast(target_col.dataType))\n",
        "                else:\n",
        "                    # If Column not present in dataframe, add column with None values.\n",
        "                    df = df.withColumn(col_name, f.lit(None).cast(target_col.dataType))\n",
        "        # If Complex datatype, i.e. Object, Array\n",
        "        else:\n",
        "            if col_name not in df.columns:\n",
        "                df = df.withColumn(col_name, f.lit(None).cast(target_col.dataType))\n",
        "            else:\n",
        "                # Generate JSON column as a Complex Type\n",
        "                df = df.withColumn(f\"{col_name}_json\", f.to_json(f.col(col_name))) \\\n",
        "                    .withColumn(col_name, f.from_json(f.col(f\"{col_name}_json\"), target_col.dataType)) \\\n",
        "                    .drop(f\"{col_name}_json\")\n",
        "            \n",
        "            # Modify the links with surrogate keys\n",
        "            if re.search('Reference$', col_name) is not None:\n",
        "                df = flatten_reference_col(df, target_col)\n",
        "    \n",
        "            if target_col.dataType.typeName() == 'array':\n",
        "                df = explode_arrays(df, sink_general_path,target_col, schema_name, table_name, extension = extension)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-10-20T09:19:57.5295046Z",
              "execution_start_time": "2023-10-20T09:19:57.353596Z",
              "livy_statement_state": "available",
              "parent_msg_id": "2c302d7b-e0b1-4bf1-b2e0-e058f1ffcd1f",
              "queued_time": "2023-10-20T09:19:53.4223516Z",
              "session_id": "32",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "spark3p3sm",
              "state": "finished",
              "statement_id": 63
            },
            "text/plain": [
              "StatementMeta(spark3p3sm, 32, 63, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def get_ext_entities_schemas(table_name = 'staffs',\n",
        "                             ext_column_name = '_ext',\n",
        "                             default_value = 'TPDM'):\n",
        "    target_schema = copy.deepcopy(schemas[table_name])\n",
        "    for col_name in target_schema.fieldNames():\n",
        "        target_col = target_schema[col_name]\n",
        "        if target_col.name == ext_column_name:\n",
        "            if target_col.dataType[0].name == default_value:\n",
        "                return target_col.dataType[0].dataType         \n",
        "                \n",
        "def flatten_ext_column(df, \n",
        "                       table_name, \n",
        "                       ext_col, \n",
        "                       inner_key,\n",
        "                       ext_inner_cols\n",
        "                       ):\n",
        "    # TODO: Assess if LastModifiedDate inclusion breaks ETL or not\n",
        "    cols = ['lakeId', 'DistrictId', 'SchoolYear', 'id_pseudonym', 'LastModifiedDate']\n",
        "    flattened_cols = ext_inner_cols#[\"educatorPreparationPrograms\"] #_ext_TX_cols[table_name]\n",
        "    dict_col = F.col(ext_col)[inner_key]\n",
        "    complex_dtype_text = str(df.select('_ext').dtypes[0][1])\n",
        "\n",
        "    exprs = [dict_col.getItem(key).alias(key) for key in flattened_cols if str(key) in complex_dtype_text]\n",
        "    try:\n",
        "        flattened_df = df.select(exprs + cols)\n",
        "    except:\n",
        "        cols = ['lakeId', 'DistrictId', 'SchoolYear', 'id_pseudonym']\n",
        "        flattened_df = df.select(exprs + cols)\n",
        "    return flattened_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-10-20T09:19:57.9019356Z",
              "execution_start_time": "2023-10-20T09:19:57.7101633Z",
              "livy_statement_state": "available",
              "parent_msg_id": "3b07a6f3-e271-4c63-8e48-1b22e6c42874",
              "queued_time": "2023-10-20T09:19:54.1022246Z",
              "session_id": "32",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "spark3p3sm",
              "state": "finished",
              "statement_id": 64
            },
            "text/plain": [
              "StatementMeta(spark3p3sm, 32, 64, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def sink_path_cleanup(destination_path):\n",
        "    pattern = re.compile(r'DistrictId=.*?/|SchoolYear=.*?/')\n",
        "    destination_path = re.sub(pattern, '', destination_path)\n",
        "\n",
        "    return destination_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-10-20T09:19:58.2586461Z",
              "execution_start_time": "2023-10-20T09:19:58.0802606Z",
              "livy_statement_state": "available",
              "parent_msg_id": "061b1617-7850-4625-b66e-12dc9ffc181c",
              "queued_time": "2023-10-20T09:19:54.6704658Z",
              "session_id": "32",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "spark3p3sm",
              "state": "finished",
              "statement_id": 65
            },
            "text/plain": [
              "StatementMeta(spark3p3sm, 32, 65, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def refine_and_explode_data(schema_name, \n",
        "                            tables_source,\n",
        "                            ext_entity,\n",
        "                            metadata, \n",
        "                            transform_mode, \n",
        "                            test_mode,\n",
        "                            items = []):\n",
        "    global districtPath,schoolYearPath\n",
        "    if items == 'All':\n",
        "        items = oea.get_folders(f\"stage2/Ingested/{tables_source}\")\n",
        "    for item in items:\n",
        "            table_name = item #sap_to_edfi_complex[item]\n",
        "            table_path = f\"{tables_source}/{item}\"\n",
        "            if not(oea.path_exists(f\"stage2/Ingested/{table_path}\")):\n",
        "                print(table_path)\n",
        "                continue            \n",
        "            logger.info(f\"Processing schema/table: {schema_name}/{table_name}\")\n",
        "            if item == 'metadata.csv':\n",
        "                logger.info('ignore metadata processing, since this is not a table to be ingested')\n",
        "            else:\n",
        "                try:\n",
        "                    if not(transform_mode):\n",
        "                        df = oea.refine(table_path, \n",
        "                                        metadata = metadata[item], \n",
        "                                        primary_key = 'id')\n",
        "                    if transform_mode:\n",
        "                        logger.info('Ed-Fi to Ed-Fi Relationship Model: ' + table_name)               \n",
        "                        source_path = f'stage2/Ingested/{table_path}'\n",
        "                        sink_general_path, sink_sensitive_path = oea.get_sink_general_sensitive_paths(source_path)\n",
        "                        \n",
        "                        sink_general_path = sink_path_cleanup(sink_general_path)\n",
        "                        sink_sensitive_path = sink_path_cleanup(sink_sensitive_path)\n",
        "\n",
        "                        df_changes = oea.get_latest_changes(source_path, sink_general_path)\n",
        "                        df_changes = df_changes.withColumn('DistrictId', F.lit(districtPath))\n",
        "                        df_changes = df_changes.withColumn('SchoolYear', F.lit(schoolYearPath))\n",
        "                        \n",
        "                        current_timestamp = datetime.now()\n",
        "                        df_changes = df_changes.withColumn('LastModifiedDate', F.lit(current_timestamp))\n",
        "                        \n",
        "                        if df_changes.count() > 0:\n",
        "                            df_pseudo, df_lookup = oea.pseudonymize(df_changes, \n",
        "                                                                    metadata,\n",
        "                                                                    transform_mode,\n",
        "                                                                    True)\n",
        "                            \n",
        "                            transform(df_pseudo, \n",
        "                                      schema_name, \n",
        "                                      table_name, \n",
        "                                      'id_pseudonym', \n",
        "                                      ext_entity, \n",
        "                                      sink_general_path,\n",
        "                                      None, \n",
        "                                      None)\n",
        "                            \n",
        "                            oea.upsert(df = df_lookup, \n",
        "                                       destination_path = sink_sensitive_path, \n",
        "                                       primary_key = 'id',\n",
        "                                       partitioning = True,\n",
        "                                       partitioning_cols = ['DistrictId', 'SchoolYear'])    \n",
        "                            oea.add_to_lake_db(source_entity_path = sink_sensitive_path,\n",
        "                                               overwrite = True,\n",
        "                                               extension = None)\n",
        "                        else:\n",
        "                            logger.info(f'No updated rows in {source_path} to process.')\n",
        "\n",
        "                except AnalysisException as e:\n",
        "                    # This means the table may have not been properly refined due to errors with the primary key not aligning with columns expected in the lookup table.\n",
        "                    logger.info(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": null,
              "execution_start_time": "2023-10-20T09:20:04.6146563Z",
              "livy_statement_state": "running",
              "parent_msg_id": "b39f4dc4-ea99-4eaa-bd7e-15735c1d0d97",
              "queued_time": "2023-10-20T09:20:04.4563285Z",
              "session_id": "32",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "spark3p3sm",
              "state": "submitted",
              "statement_id": 67
            },
            "text/plain": [
              "StatementMeta(spark3p3sm, 32, 67, Submitted, Running)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-10-20 09:20:05,458 - OEA - INFO - Processing schema/table: ed-fi/absenceEventCategoryDescriptors\n",
            "2023-10-20 09:20:05,459 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: absenceEventCategoryDescriptors\n",
            "2023-10-20 09:20:07,373 - OEA - INFO - No updated rows in stage2/Ingested/Ed-Fi/5.2/DistrictId=All/SchoolYear=2023/ed-fi/absenceEventCategoryDescriptors to process.\n",
            "2023-10-20 09:20:07,421 - OEA - INFO - Processing schema/table: ed-fi/academicHonorCategoryDescriptors\n",
            "2023-10-20 09:20:07,421 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: academicHonorCategoryDescriptors\n",
            "2023-10-20 09:20:09,576 - OEA - INFO - No updated rows in stage2/Ingested/Ed-Fi/5.2/DistrictId=All/SchoolYear=2023/ed-fi/academicHonorCategoryDescriptors to process.\n",
            "2023-10-20 09:20:09,599 - OEA - INFO - Processing schema/table: ed-fi/academicSubjectDescriptors\n",
            "2023-10-20 09:20:09,599 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: academicSubjectDescriptors\n",
            "2023-10-20 09:20:12,276 - OEA - INFO - No updated rows in stage2/Ingested/Ed-Fi/5.2/DistrictId=All/SchoolYear=2023/ed-fi/academicSubjectDescriptors to process.\n",
            "2023-10-20 09:20:12,301 - OEA - INFO - Processing schema/table: ed-fi/accommodationDescriptors\n",
            "2023-10-20 09:20:12,301 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: accommodationDescriptors\n",
            "2023-10-20 09:20:14,261 - OEA - INFO - No updated rows in stage2/Ingested/Ed-Fi/5.2/DistrictId=All/SchoolYear=2023/ed-fi/accommodationDescriptors to process.\n",
            "2023-10-20 09:20:14,283 - OEA - INFO - Processing schema/table: ed-fi/accountClassificationDescriptors\n",
            "2023-10-20 09:20:14,283 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: accountClassificationDescriptors\n",
            "2023-10-20 09:20:16,310 - OEA - INFO - No updated rows in stage2/Ingested/Ed-Fi/5.2/DistrictId=All/SchoolYear=2023/ed-fi/accountClassificationDescriptors to process.\n",
            "2023-10-20 09:20:16,348 - OEA - INFO - Processing schema/table: ed-fi/accountCodes\n",
            "2023-10-20 09:20:16,348 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: accountCodes\n",
            "2023-10-20 09:20:17,902 - OEA - INFO - Writing Main Table - accountCodes\n",
            "2023-10-20 09:20:18,116 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:20:42,825 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:20:49,703 - OEA - INFO - Processing schema/table: ed-fi/accountabilityRatings\n",
            "2023-10-20 09:20:49,704 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: accountabilityRatings\n",
            "2023-10-20 09:20:54,034 - OEA - INFO - Writing Main Table - accountabilityRatings\n",
            "2023-10-20 09:20:54,227 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:21:03,094 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:21:09,411 - OEA - INFO - Processing schema/table: ed-fi/accounts\n",
            "2023-10-20 09:21:09,412 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: accounts\n",
            "2023-10-20 09:21:13,252 - OEA - INFO - Writing Child Table - accounts_accountCodes\n",
            "2023-10-20 09:21:13,412 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:21:20,637 - OEA - INFO - Writing Main Table - accounts\n",
            "2023-10-20 09:21:20,778 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:21:27,598 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:21:33,124 - OEA - INFO - Processing schema/table: ed-fi/achievementCategoryDescriptors\n",
            "2023-10-20 09:21:33,124 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: achievementCategoryDescriptors\n",
            "2023-10-20 09:21:36,896 - OEA - INFO - Writing Main Table - achievementCategoryDescriptors\n",
            "2023-10-20 09:21:37,042 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:21:43,471 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:21:48,708 - OEA - INFO - Processing schema/table: ed-fi/additionalCreditTypeDescriptors\n",
            "2023-10-20 09:21:48,708 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: additionalCreditTypeDescriptors\n",
            "2023-10-20 09:21:52,139 - OEA - INFO - Writing Main Table - additionalCreditTypeDescriptors\n",
            "2023-10-20 09:21:52,330 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:21:59,243 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:22:04,438 - OEA - INFO - Processing schema/table: ed-fi/addressTypeDescriptors\n",
            "2023-10-20 09:22:04,439 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: addressTypeDescriptors\n",
            "2023-10-20 09:22:07,744 - OEA - INFO - Writing Main Table - addressTypeDescriptors\n",
            "2023-10-20 09:22:07,918 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:22:14,003 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:22:19,367 - OEA - INFO - Processing schema/table: ed-fi/administrationEnvironmentDescriptors\n",
            "2023-10-20 09:22:19,367 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: administrationEnvironmentDescriptors\n",
            "2023-10-20 09:22:22,404 - OEA - INFO - Writing Main Table - administrationEnvironmentDescriptors\n",
            "2023-10-20 09:22:22,620 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:22:28,592 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:22:33,616 - OEA - INFO - Processing schema/table: ed-fi/administrativeFundingControlDescriptors\n",
            "2023-10-20 09:22:33,617 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: administrativeFundingControlDescriptors\n",
            "2023-10-20 09:22:37,476 - OEA - INFO - Writing Main Table - administrativeFundingControlDescriptors\n",
            "2023-10-20 09:22:37,633 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:22:43,379 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:22:48,726 - OEA - INFO - Processing schema/table: ed-fi/ancestryEthnicOriginDescriptors\n",
            "2023-10-20 09:22:48,726 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: ancestryEthnicOriginDescriptors\n",
            "2023-10-20 09:22:52,020 - OEA - INFO - Writing Main Table - ancestryEthnicOriginDescriptors\n",
            "2023-10-20 09:22:52,173 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:22:58,277 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:23:03,863 - OEA - INFO - Processing schema/table: ed-fi/assessmentCategoryDescriptors\n",
            "2023-10-20 09:23:03,863 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: assessmentCategoryDescriptors\n",
            "2023-10-20 09:23:07,359 - OEA - INFO - Writing Main Table - assessmentCategoryDescriptors\n",
            "2023-10-20 09:23:07,550 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:23:13,213 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:23:18,522 - OEA - INFO - Processing schema/table: ed-fi/assessmentIdentificationSystemDescriptors\n",
            "2023-10-20 09:23:18,523 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: assessmentIdentificationSystemDescriptors\n",
            "2023-10-20 09:23:21,838 - OEA - INFO - Writing Main Table - assessmentIdentificationSystemDescriptors\n",
            "2023-10-20 09:23:21,994 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:23:27,188 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:23:32,430 - OEA - INFO - Processing schema/table: ed-fi/assessmentItemCategoryDescriptors\n",
            "2023-10-20 09:23:32,430 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: assessmentItemCategoryDescriptors\n",
            "2023-10-20 09:23:35,605 - OEA - INFO - Writing Main Table - assessmentItemCategoryDescriptors\n",
            "2023-10-20 09:23:35,789 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:23:41,942 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:23:47,097 - OEA - INFO - Processing schema/table: ed-fi/assessmentItemResultDescriptors\n",
            "2023-10-20 09:23:47,097 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: assessmentItemResultDescriptors\n",
            "2023-10-20 09:23:50,678 - OEA - INFO - Writing Main Table - assessmentItemResultDescriptors\n",
            "2023-10-20 09:23:50,852 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:23:56,368 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:24:01,950 - OEA - INFO - Processing schema/table: ed-fi/assessmentItems\n",
            "2023-10-20 09:24:01,950 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: assessmentItems\n",
            "2023-10-20 09:24:05,892 - OEA - INFO - Writing Child Table - assessmentItems_learningStandards\n",
            "2023-10-20 09:24:05,997 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:24:12,816 - OEA - INFO - Writing Child Table - assessmentItems_possibleResponses\n",
            "2023-10-20 09:24:12,928 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:24:18,702 - OEA - INFO - Writing Main Table - assessmentItems\n",
            "2023-10-20 09:24:18,868 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:24:25,698 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:24:30,654 - OEA - INFO - Processing schema/table: ed-fi/assessmentPeriodDescriptors\n",
            "2023-10-20 09:24:30,654 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: assessmentPeriodDescriptors\n",
            "2023-10-20 09:24:33,932 - OEA - INFO - Writing Main Table - assessmentPeriodDescriptors\n",
            "2023-10-20 09:24:34,122 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:24:39,912 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:24:45,304 - OEA - INFO - Processing schema/table: ed-fi/assessmentReportingMethodDescriptors\n",
            "2023-10-20 09:24:45,304 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: assessmentReportingMethodDescriptors\n",
            "2023-10-20 09:24:48,391 - OEA - INFO - Writing Main Table - assessmentReportingMethodDescriptors\n",
            "2023-10-20 09:24:48,597 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:24:54,690 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:25:00,424 - OEA - INFO - Processing schema/table: ed-fi/assessmentScoreRangeLearningStandards\n",
            "2023-10-20 09:25:00,424 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: assessmentScoreRangeLearningStandards\n",
            "2023-10-20 09:25:03,382 - OEA - INFO - Writing Child Table - assessmentScoreRangeLearningStandards_learningStandards\n",
            "2023-10-20 09:25:03,507 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:25:10,213 - OEA - INFO - Writing Main Table - assessmentScoreRangeLearningStandards\n",
            "2023-10-20 09:25:10,346 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:25:16,357 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:25:21,506 - OEA - INFO - Processing schema/table: ed-fi/assessments\n",
            "2023-10-20 09:25:21,507 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: assessments\n",
            "2023-10-20 09:25:24,666 - OEA - INFO - Writing Child Table - assessments_academicSubjects\n",
            "2023-10-20 09:25:24,783 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:25:31,705 - OEA - INFO - Writing Child Table - assessments_assessedGradeLevels\n",
            "2023-10-20 09:25:31,822 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:25:37,327 - OEA - INFO - Writing Child Table - assessments_identificationCodes\n",
            "2023-10-20 09:25:37,459 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:25:43,302 - OEA - INFO - Writing Child Table - assessments_languages\n",
            "2023-10-20 09:25:43,417 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:25:48,793 - OEA - INFO - Writing Child Table - assessments_performanceLevels\n",
            "2023-10-20 09:25:48,931 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:25:55,835 - OEA - INFO - Writing Child Table - assessments_platformTypes\n",
            "2023-10-20 09:25:55,950 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:26:01,447 - OEA - INFO - Writing Child Table - assessments_programs\n",
            "2023-10-20 09:26:01,573 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:26:07,656 - OEA - INFO - Writing Child Table - assessments_scores\n",
            "2023-10-20 09:26:07,827 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:26:13,569 - OEA - INFO - Writing Child Table - assessments_sections\n",
            "2023-10-20 09:26:13,712 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:26:19,281 - OEA - INFO - Writing Main Table - assessments\n",
            "2023-10-20 09:26:19,536 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:26:25,921 - OEA - INFO - Writing EXT Table - assessments\n",
            "2023-10-20 09:26:26,059 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:26:31,894 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:26:37,215 - OEA - INFO - Processing schema/table: ed-fi/attemptStatusDescriptors\n",
            "2023-10-20 09:26:37,215 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: attemptStatusDescriptors\n",
            "2023-10-20 09:26:40,510 - OEA - INFO - Writing Main Table - attemptStatusDescriptors\n",
            "2023-10-20 09:26:40,715 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:26:47,905 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:26:53,485 - OEA - INFO - Processing schema/table: ed-fi/attendanceEventCategoryDescriptors\n",
            "2023-10-20 09:26:53,485 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: attendanceEventCategoryDescriptors\n",
            "2023-10-20 09:26:57,184 - OEA - INFO - Writing Main Table - attendanceEventCategoryDescriptors\n",
            "2023-10-20 09:26:57,340 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:27:03,186 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:27:08,562 - OEA - INFO - Processing schema/table: ed-fi/behaviorDescriptors\n",
            "2023-10-20 09:27:08,562 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: behaviorDescriptors\n",
            "2023-10-20 09:27:11,884 - OEA - INFO - Writing Main Table - behaviorDescriptors\n",
            "2023-10-20 09:27:12,031 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:27:18,152 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:27:23,122 - OEA - INFO - Processing schema/table: ed-fi/bellSchedules\n",
            "2023-10-20 09:27:23,123 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: bellSchedules\n",
            "2023-10-20 09:27:27,046 - OEA - INFO - Writing Child Table - bellSchedules_classPeriods\n",
            "2023-10-20 09:27:27,181 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:27:33,888 - OEA - INFO - Writing Child Table - bellSchedules_dates\n",
            "2023-10-20 09:27:33,986 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:27:43,821 - OEA - INFO - Writing Child Table - bellSchedules_gradeLevels\n",
            "2023-10-20 09:27:43,952 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:27:49,165 - OEA - INFO - Writing Main Table - bellSchedules\n",
            "2023-10-20 09:27:49,313 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:27:56,320 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:28:01,547 - OEA - INFO - Processing schema/table: ed-fi/calendarDates\n",
            "2023-10-20 09:28:01,547 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: calendarDates\n",
            "2023-10-20 09:28:04,484 - OEA - INFO - Writing Child Table - calendarDates_calendarEvents\n",
            "2023-10-20 09:28:04,687 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:28:12,400 - OEA - INFO - Writing Main Table - calendarDates\n",
            "2023-10-20 09:28:12,526 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:28:18,952 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:28:24,213 - OEA - INFO - Processing schema/table: ed-fi/calendarEventDescriptors\n",
            "2023-10-20 09:28:24,214 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: calendarEventDescriptors\n",
            "2023-10-20 09:28:27,653 - OEA - INFO - Writing Main Table - calendarEventDescriptors\n",
            "2023-10-20 09:28:27,773 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:28:34,166 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:28:39,310 - OEA - INFO - Processing schema/table: ed-fi/calendarTypeDescriptors\n",
            "2023-10-20 09:28:39,311 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: calendarTypeDescriptors\n",
            "2023-10-20 09:28:42,416 - OEA - INFO - Writing Main Table - calendarTypeDescriptors\n",
            "2023-10-20 09:28:42,560 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:28:49,146 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:28:54,444 - OEA - INFO - Processing schema/table: ed-fi/calendars\n",
            "2023-10-20 09:28:54,445 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: calendars\n",
            "2023-10-20 09:28:57,923 - OEA - INFO - Writing Child Table - calendars_gradeLevels\n",
            "2023-10-20 09:28:58,010 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:29:05,075 - OEA - INFO - Writing Main Table - calendars\n",
            "2023-10-20 09:29:05,302 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:29:11,129 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:29:15,933 - OEA - INFO - Processing schema/table: ed-fi/careerPathwayDescriptors\n",
            "2023-10-20 09:29:15,933 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: careerPathwayDescriptors\n",
            "2023-10-20 09:29:19,718 - OEA - INFO - Writing Main Table - careerPathwayDescriptors\n",
            "2023-10-20 09:29:19,886 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:29:25,706 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:29:31,183 - OEA - INFO - Processing schema/table: ed-fi/charterApprovalAgencyTypeDescriptors\n",
            "2023-10-20 09:29:31,184 - OEA - INFO - Ed-Fi to Ed-Fi Relationship Model: charterApprovalAgencyTypeDescriptors\n",
            "2023-10-20 09:29:35,124 - OEA - INFO - Writing Main Table - charterApprovalAgencyTypeDescriptors\n",
            "2023-10-20 09:29:35,392 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n",
            "2023-10-20 09:29:41,779 - OEA - INFO - Writing partitioned delta lake - partitioned by - DistrictId, SchoolYear\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "schema_name = 'ed-fi'\n",
        "ext_entity = 'TPDM'\n",
        "test_mode = False\n",
        "transform_mode = True\n",
        "tables_source = f'{moduleName}/{apiVersion}/DistrictId={districtPath}/SchoolYear={schoolYearPath}/{schema_name}'\n",
        "transform_items = edfiEntities \n",
        "metadata = oea.get_metadata_from_url(metadataUrl)\n",
        "\n",
        "refine_and_explode_data(schema_name, \n",
        "                        tables_source,\n",
        "                        ext_entity,\n",
        "                        metadata,\n",
        "                        transform_mode, \n",
        "                        test_mode,\n",
        "                        items = transform_items)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
