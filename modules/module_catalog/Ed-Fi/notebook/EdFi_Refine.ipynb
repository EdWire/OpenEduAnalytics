{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import pyspark.sql.functions as f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "workspace = 'dev'\n",
        "apiVersion = \"5.2\"\n",
        "moduleName = \"Ed-Fi\"\n",
        "schoolYear = None\n",
        "districtId = None\n",
        "#metadataUrl = \"https://github.com/ayunav/OpenEduAnalytics/raw/main/modules/module_catalog/Ed-Fi/utils/Metadata.csv\"\n",
        "\n",
        "swagger_url = \"https://api.edgraph.com/edfi/v5.2/saas/tx/api/metadata/data/v3/edfi-swagger.json\"\n",
        "metadataUrl = \"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Ed-Fi/utils/Metadata.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%run dev_v0_1_OEA_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%run OEA_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# 1) set the workspace (this determines where in the data lake you'll be writing to and reading from).\n",
        "# You can work in 'dev', 'prod', or a sandbox with any name you choose.\n",
        "# For example, Sam the developer can create a 'sam' workspace and expect to find his datasets in the data lake under oea/sandboxes/sam\n",
        "workspace = 'dev'\n",
        "oea.set_workspace(workspace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "schema_gen = OpenAPIUtil(swagger_url)\n",
        "schemas = schema_gen.create_spark_schemas()\n",
        "primitive_datatypes = ['timestamp', 'date', 'decimal', 'boolean', 'integer', 'string', 'long']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def get_descriptor_schema(descriptor):\n",
        "    fields = []\n",
        "    fields.append(StructField('_etag',LongType(), True))\n",
        "    fields.append(StructField(f\"{descriptor[:-1]}Id\", IntegerType(), True))\n",
        "    fields.append(StructField('codeValue',StringType(), True))\n",
        "    fields.append(StructField('description',StringType(), True))\n",
        "    fields.append(StructField('id',StringType(), True))\n",
        "    fields.append(StructField('namespace',StringType(), True))\n",
        "    fields.append(StructField('shortDescription',StringType(), True))\n",
        "    return StructType(fields)\n",
        "\n",
        "def get_descriptor_metadata(descriptor):\n",
        "    return [['_etag', 'long', 'no-op'],\n",
        "            [f\"{descriptor[:-1]}Id\", 'integer', 'hash'],\n",
        "            ['codeValue','string', 'no-op'],\n",
        "            ['description','string', 'no-op'],\n",
        "            ['id','string', 'no-op'],\n",
        "            ['namespace','string', 'no-op'],\n",
        "            ['shortDescription','string', 'no-op']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def has_column(df, col):\n",
        "    try:\n",
        "        df[col]\n",
        "        return True\n",
        "    except AnalysisException:\n",
        "        return False\n",
        "\n",
        "def modify_descriptor_value(df, col_name):\n",
        "    if col_name in df.columns:\n",
        "        # TODO: @Abhinav (Edit: Now @Viraj), I do not see where you made the changes to use the descriptorId instead of Namespace/CodeValue\n",
        "        df = df.withColumn(f\"{col_name}LakeId\", f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.regexp_replace(col_name, '#', '_')))\n",
        "        df = df.drop(col_name)\n",
        "    else:\n",
        "        df = df.withColumn(f\"{col_name}LakeId\", f.lit(None).cast(\"String\"))\n",
        "\n",
        "    return df\n",
        "\n",
        "def flatten_reference_col(df, target_col):\n",
        "    col_prefix = target_col.name.replace('Reference', '')\n",
        "    df = df.withColumn(f\"{col_prefix}LakeId\", f.when(f.col(target_col.name).isNotNull(), f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.split(f.col(f'{target_col.name}.link.href'), '/').getItem(3))))\n",
        "    df = df.drop(target_col.name)\n",
        "    return df\n",
        "\n",
        "def modify_references_and_descriptors(df, target_col):\n",
        "    for ref_col in [x for x in df.columns if re.search('Reference$', x) is not None]:\n",
        "        df = flatten_reference_col(df, target_col.dataType.elementType[ref_col])\n",
        "    for desc_col in [x for x in df.columns if re.search('Descriptor$', x) is not None]:\n",
        "        df = modify_descriptor_value(df, desc_col)\n",
        "    return df\n",
        "\n",
        "def explode_arrays(df, sink_general_path,target_col, schema_name, table_name):\n",
        "    cols = ['lakeId', 'DistrictId', 'SchoolYear']\n",
        "    child_df = df.select(cols + [target_col.name])\n",
        "    child_df = child_df.withColumn(\"exploded\", f.explode(target_col.name)).drop(target_col.name).select(cols + ['exploded.*'])\n",
        "\n",
        "    # TODO: It looks like te {target_col.name}LakeId column is not addedd to the child entities\n",
        "    #       We should use LakeId suffix when using the \"id\" column from the parent and HKey suffix when creating a Hash Key based on composite key columns\n",
        "    identity_cols = [x.name for x in target_col.dataType.elementType.fields if 'x-Ed-Fi-isIdentity' in x.metadata].sort()\n",
        "    if(identity_cols is not None and len(identity_cols) > 0):\n",
        "        child_df = child_df.withColumn(f\"{target_col.name}LakeId\", f.concat(f.col('DistrictId'), f.lit('_'), f.col('SchoolYear'), f.lit('_'), *[f.concat(f.col(x), f.lit('_')) for x in identity_cols]))\n",
        "    \n",
        "    # IMPORTANT: We must modify Reference and Descriptor columns for child columns \"first\". \n",
        "    # This must be done \"after\" the composite key from identity_cols has been created otherwise the columns are renamed and will not be found by identity_cols.\n",
        "    # This must be done \"before\" the grand_child is exploded below\n",
        "    child_df = modify_references_and_descriptors(child_df, target_col)\n",
        "\n",
        "    for array_sub_col in [x for x in target_col.dataType.elementType.fields if x.dataType.typeName() == 'array' ]:\n",
        "        grand_child_df = child_df.withColumn('exploded', f.explode(array_sub_col.name)).select(child_df.columns + ['exploded.*']).drop(array_sub_col.name)\n",
        "        \n",
        "        # Modifying Reference and Descriptor columns for the grand_child array\n",
        "        grand_child_df = modify_references_and_descriptors(grand_child_df, array_sub_col)\n",
        "\n",
        "        # TODO: Pseudonimize and Write to Sensitive folder for child arrays?\n",
        "        logger.info(f\"Writing Grand Child Table - {table_name}_{target_col.name}_{array_sub_col.name}\")\n",
        "        oea.upsert(grand_child_df, \n",
        "                   f\"{sink_general_path}_{target_col.name}_{array_sub_col.name}\", \n",
        "                   'lakeId') \n",
        "        oea.add_to_lake_db(f\"{sink_general_path}_{target_col.name}_{array_sub_col.name}\")\n",
        "        #grand_child_df.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').save(oea.to_url(f\"{sink_general_path}_{target_col.name}_{array_sub_col.name}\"))\n",
        "\n",
        "    # TODO: Pseudonimize and Write to Sensitive folder for child arrays?\n",
        "    logger.info(f\"Writing Child Table - {table_name}_{target_col.name}\")\n",
        "    oea.upsert(child_df, \n",
        "               f\"{sink_general_path}_{target_col.name}\", \n",
        "              'lakeId') \n",
        "    oea.add_to_lake_db(f\"{sink_general_path}_{target_col.name}\")\n",
        "    #child_df.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').save(oea.to_url(f\"{sink_general_path}_{target_col.name}\"))\n",
        "\n",
        "    # Drop array column from parent entity\n",
        "    df = df.drop(target_col.name)\n",
        "    return df\n",
        "\n",
        "def transform(df, \n",
        "              schema_name, \n",
        "              table_name, \n",
        "              primary_key,\n",
        "              ext_entity,\n",
        "              sink_general_path,\n",
        "              parent_schema_name, \n",
        "              parent_table_name):\n",
        "    # TODO: Pseudominization Pending\n",
        "    if re.search('Descriptors$', table_name) is None:\n",
        "        # Use Deep Copy otherwise the schemas object also gets modified every time target_schema is modified\n",
        "        target_schema = copy.deepcopy(schemas[table_name])\n",
        "        # Add primary key\n",
        "        if has_column(df, primary_key):\n",
        "            df = df.withColumn('lakeId', f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.col(primary_key)).cast(\"String\"))\n",
        "        else:\n",
        "            df = df.withColumn('lakeId', f.lit(None).cast(\"String\"))\n",
        "    else:\n",
        "        target_schema = get_descriptor_schema(table_name)\n",
        "        # Add primary key\n",
        "        if has_column(df, primary_key):\n",
        "            # TODO: @Abhinav (Edit: Now @Viraj), I do not see where you made the changes to use the descriptorId instead of Namespace/CodeValue\n",
        "            df = df.withColumn('lakeId', f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.col('namespace'), f.col('codeValue')).cast(\"String\"))\n",
        "        else:\n",
        "            df = df.withColumn('lakeId', f.lit(None).cast(\"String\"))\n",
        "\n",
        "    target_schema = target_schema.add(StructField('DistrictId', StringType()))\\\n",
        "                                 .add(StructField('SchoolYear', StringType()))\\\n",
        "                                 .add(StructField('LastModifiedDate', TimestampType()))\n",
        "\n",
        "    df = transform_sub_module(df, target_schema, sink_general_path, schema_name, table_name)\n",
        "    logger.info(f\"Writing Main Table - {table_name}\")\n",
        "    oea.upsert(df, \n",
        "               f\"{sink_general_path}\", \n",
        "              'lakeId') \n",
        "    #df.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').save(oea.to_url(sink_general_path))\n",
        "    oea.add_to_lake_db(sink_general_path)\n",
        "\n",
        "    if '_ext' in df.columns:\n",
        "        target_schema = get_ext_entities_schemas(table_name = table_name,\n",
        "                                                 ext_column_name = '_ext',\n",
        "                                                 default_value = ext_entity)\n",
        "        df = flatten_ext_column(df = df, \n",
        "                                table_name = table_name, \n",
        "                                ext_col = '_ext', \n",
        "                                inner_key = ext_entity,\n",
        "                                ext_inner_cols = target_schema.fieldNames())\n",
        "        sink_general_path = sink_general_path.replace('/ed-fi/', f'/{ext_entity.lower()}/')\n",
        "        df = transform_sub_module(df, \n",
        "                                  target_schema, \n",
        "                                  sink_general_path, \n",
        "                                  schema_name,\n",
        "                                  table_name)\n",
        "\n",
        "        logger.info(f\"Writing EXT Table - {table_name}\")\n",
        "        oea.upsert(df, \n",
        "               f\"{sink_general_path}\", \n",
        "              'lakeId') \n",
        "        #df.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').save(oea.to_url(sink_general_path))\n",
        "        oea.add_to_lake_db(sink_general_path)\n",
        "        \n",
        "\n",
        "#df = spark.read.format('delta').load(f\"{stage2_ingested}/ed-fi/absenceEventCategoryDescriptors\")\n",
        "#df = transform(df, \"ed-fi\", \"absenceEventCategoryDescriptors\", None, None)\n",
        "\n",
        "def transform_sub_module(df, target_schema, sink_general_path, schema_name, table_name):\n",
        "    for col_name in target_schema.fieldNames():\n",
        "        target_col = target_schema[col_name]\n",
        "        # If Primitive datatype, i.e String, Bool, Integer, etc.abs\n",
        "        # Note: Descriptor is a String therefore is a Primitive datatype\n",
        "        if target_col.dataType.typeName() in primitive_datatypes:\n",
        "            # If it is a Descriptor\n",
        "            if re.search('Descriptor$', col_name) is not None:\n",
        "                df = modify_descriptor_value(df, col_name)\n",
        "            else:\n",
        "                if col_name in df.columns:\n",
        "                    # Casting columns to primitive data types\n",
        "                    df = df.withColumn(col_name, f.col(col_name).cast(target_col.dataType))\n",
        "                else:\n",
        "                    # If Column not present in dataframe, add column with None values.\n",
        "                    df = df.withColumn(col_name, f.lit(None).cast(target_col.dataType))\n",
        "        # If Complex datatype, i.e. Object, Array\n",
        "        else:\n",
        "            if col_name not in df.columns:\n",
        "                df = df.withColumn(col_name, f.lit(None).cast(target_col.dataType))\n",
        "            else:\n",
        "                # Generate JSON column as a Complex Type\n",
        "                df = df.withColumn(f\"{col_name}_json\", f.to_json(f.col(col_name))) \\\n",
        "                    .withColumn(col_name, f.from_json(f.col(f\"{col_name}_json\"), target_col.dataType)) \\\n",
        "                    .drop(f\"{col_name}_json\")\n",
        "            \n",
        "            # Modify the links with surrogate keys\n",
        "            if re.search('Reference$', col_name) is not None:\n",
        "                df = flatten_reference_col(df, target_col)\n",
        "    \n",
        "            if target_col.dataType.typeName() == 'array':\n",
        "                df = explode_arrays(df, sink_general_path,target_col, schema_name, table_name)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def get_ext_entities_schemas(table_name = 'staffs',\n",
        "                             ext_column_name = '_ext',\n",
        "                             default_value = 'TPDM'):\n",
        "    target_schema = copy.deepcopy(schemas[table_name])\n",
        "    for col_name in target_schema.fieldNames():\n",
        "        target_col = target_schema[col_name]\n",
        "        if target_col.name == ext_column_name:\n",
        "            if target_col.dataType[0].name == default_value:\n",
        "                return target_col.dataType[0].dataType         \n",
        "                \n",
        "def flatten_ext_column(df, \n",
        "                       table_name, \n",
        "                       ext_col, \n",
        "                       inner_key,\n",
        "                       ext_inner_cols\n",
        "                       ):\n",
        "    #TODO: Modify the complex sub type field name logic\n",
        "    cols = ['lakeId', 'DistrictId', 'SchoolYear', 'id_pseudonym']\n",
        "    flattened_cols = ext_inner_cols#[\"educatorPreparationPrograms\"] #_ext_TX_cols[table_name]\n",
        "    dict_col = F.col(ext_col)[inner_key]\n",
        "    complex_dtype_text = str(df.select('_ext').dtypes[0][1])\n",
        "\n",
        "    exprs = [dict_col.getItem(key).alias(key) for key in flattened_cols if str(key) in complex_dtype_text]\n",
        "    flattened_df = df.select(exprs + cols)\n",
        "    return flattened_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def refine_and_explode_data(schema_name, \n",
        "                            tables_source,\n",
        "                            ext_entity, \n",
        "                            transform_mode, \n",
        "                            test_mode,\n",
        "                            test_items = []):\n",
        "    global districtPath,schoolYearPath\n",
        "    items = oea.get_folders(f\"stage2/Ingested/{tables_source}\")\n",
        "    if test_mode:\n",
        "        if test_items != 'All':\n",
        "            items = test_items\n",
        "    for item in items:\n",
        "            table_name = item #sap_to_edfi_complex[item]\n",
        "            table_path = f\"{tables_source}/{item}\"\n",
        "            \n",
        "            logger.info(f\"Processing schema/table: {schema_name}/{table_name}\")\n",
        "            if item == 'metadata.csv':\n",
        "                logger.info('ignore metadata processing, since this is not a table to be ingested')\n",
        "            else:\n",
        "                try:\n",
        "                    if not(transform_mode):\n",
        "                        df = oea.refine(table_path, \n",
        "                                        metadata = metadata[item], \n",
        "                                        primary_key = 'id')\n",
        "                    if transform_mode:\n",
        "                        logger.info('Ed-Fi to Ed-Fi Relationship Model: ' + table_name)               \n",
        "                        source_path = f'stage2/Ingested/{table_path}'\n",
        "                        sink_general_path, sink_sensitive_path = oea.get_sink_general_sensitive_paths(source_path)\n",
        "\n",
        "                        df_changes = oea.get_latest_changes(source_path, sink_general_path)\n",
        "                        df_changes = df_changes.withColumn('DistrictId', F.lit(districtPath))\n",
        "                        df_changes = df_changes.withColumn('SchoolYear', F.lit(schoolYearPath))\n",
        "                        \n",
        "                        if df_changes.count() > 0:\n",
        "                            df_pseudo, df_lookup = oea.pseudonymize(df_changes, \n",
        "                                                                    metadata, \n",
        "                                                                    transform_mode = True, \n",
        "                                                                    primary_key = 'id')\n",
        "                            \n",
        "                            transform(df_pseudo, schema_name, table_name, \n",
        "                                      'id_pseudonym', ext_entity, sink_general_path,\n",
        "                                      None, None)\n",
        "                            \n",
        "                            oea.upsert(df_lookup, sink_sensitive_path, 'id')    \n",
        "                            oea.add_to_lake_db(sink_sensitive_path)\n",
        "                        else:\n",
        "                            logger.info(f'No updated rows in {source_path} to process.')\n",
        "\n",
        "                except AnalysisException as e:\n",
        "                    # This means the table may have not been properly refined due to errors with the primary key not aligning with columns expected in the lookup table.\n",
        "                    logger.info(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "metadata = oea.get_metadata_from_url(metadataUrl)\n",
        "districtPath = districtId if districtId != None else \"All\"\n",
        "schoolYearPath = schoolYear if schoolYear != None else \"All\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "schema_name = 'ed-fi'\n",
        "ext_entity = 'TPDM'\n",
        "test_mode = False\n",
        "transform_mode = True\n",
        "tables_source = f'{moduleName}/{apiVersion}/DistrictId={districtPath}/SchoolYear={schoolYearPath}/{schema_name}'\n",
        "transform_items = 'All'#['staffs', 'students', 'staffEducationOrganizationAssignmentAssociations']\n",
        "\n",
        "refine_and_explode_data(schema_name, \n",
        "                        tables_source,\n",
        "                        ext_entity,\n",
        "                        transform_mode, \n",
        "                        test_mode,\n",
        "                        test_items = transform_items)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
