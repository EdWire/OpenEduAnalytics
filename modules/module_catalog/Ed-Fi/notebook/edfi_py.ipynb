{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\r\n",
        "import json\r\n",
        "import uuid\r\n",
        "from requests.auth import HTTPBasicAuth\r\n",
        "from datetime import datetime\r\n",
        "import logging\r\n",
        "import csv\r\n",
        "import pandas as pd\r\n",
        "from io import StringIO\r\n",
        "\r\n",
        "logger = logging.getLogger('EdFiClient')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#General parameters\r\n",
        "workspace = \"dev\"\r\n",
        "\r\n",
        "#EdFi specific parameters\r\n",
        "# kvName = \"kv-oea-hisddev\"\r\n",
        "# authUrl = \"https://api.edgraph.dev/edfi/v5.2/saas/5eb775fb-4eff-4889-9eae-3919b7a2d321/oauth/token\"\r\n",
        "# dataManagementUrl = \"https://api.edgraph.dev/edfi/v5.2/saas/data/v3/5eb775fb-4eff-4889-9eae-3919b7a2d321/2011\"\r\n",
        "# changeQueriesUrl = \"https://api.edgraph.dev/edfi/v5.2/saas/changequeries/v1/5eb775fb-4eff-4889-9eae-3919b7a2d321/2011\" \r\n",
        "# dependenciesUrl = \"https://api.edgraph.dev/edfi/v5.2/saas/metadata/data/v3/5eb775fb-4eff-4889-9eae-3919b7a2d321/2011/dependencies\"\r\n",
        "# apiVersion = \"5.2\"\r\n",
        "# batchLimit = 100\r\n",
        "# moduleName = \"Ed-Fi\"\r\n",
        "\r\n",
        "# minChangeVer = None\r\n",
        "# maxChangeVer = None\r\n",
        "\r\n",
        "# schoolYear = None\r\n",
        "# districtId = None\r\n",
        "\r\n",
        "# metadataUrl = \"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Ed-Fi/utils/Metadata.csv\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run OEA_py"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oea.set_workspace(workspace)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EdFiOEAChild(OEA):\r\n",
        "    \"\"\" \r\n",
        "    NOTE: This class inherits features from the base class OEA and therefore,\r\n",
        "    should be created / executed after running the notebook OEA_py\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, workspace='dev', logging_level=logging.INFO, storage_account=None, keyvault=None, timezone=None):\r\n",
        "        # Call the base class constructor to initialize inherited attributes\r\n",
        "        super().__init__(workspace, logging_level, storage_account, keyvault, timezone)\r\n",
        "\r\n",
        "    def upsert(self, df, destination_path, primary_key='id', partitioning=False, partitioning_cols = []):\r\n",
        "        \"\"\" Upserts the data in the given dataframe into the specified destination using the given primary_key_column to identify the updates.\r\n",
        "            If there is no delta table found in the destination_path, one will be created.    \r\n",
        "        \"\"\"\r\n",
        "        destination_url = self.to_url(destination_path)\r\n",
        "        df = self.fix_column_names(df)\r\n",
        "\r\n",
        "        if partitioning: \r\n",
        "            df = df.dropDuplicates([primary_key] + partitioning_cols)\r\n",
        "        else:\r\n",
        "            df = df.dropDuplicates([primary_key])\r\n",
        "        if DeltaTable.isDeltaTable(spark, destination_url):\r\n",
        "            delta_table_sink = DeltaTable.forPath(spark, destination_url)\r\n",
        "            \r\n",
        "            if partitioning:\r\n",
        "                #TODO: Generalize for arbitrary partitioning columns\r\n",
        "                if (sorted(partitioning_cols) == ['DistrictId', 'SchoolYear']) or (len(partitioning_cols) == 0):\r\n",
        "                    # Assumption: Each DF should have constant DistrictId and SchoolYear per run\r\n",
        "                    partitioning_cols = ['DistrictId', 'SchoolYear']\r\n",
        "                    if (df.select('DistrictId').first() and df.select('DistrictId').first()):\r\n",
        "                        DistrictId = df.select('DistrictId').first()[0]\r\n",
        "                        SchoolYear = df.select('SchoolYear').first()[0]\r\n",
        "                        destination_partition_url = self.to_url(f\"{destination_path}/DistrictId={DistrictId}/SchoolYear={SchoolYear}\")\r\n",
        "                        if DeltaTable.isDeltaTable(spark, destination_partition_url):\r\n",
        "                            logger.info('Upsert by Partitions + PK Cols')\r\n",
        "                            delta_table_sink.alias('sink').merge(df.alias('updates'), f'sink.DistrictId = updates.DistrictId AND sink.SchoolYear = updates.SchoolYear AND sink.{primary_key} = updates.{primary_key}').whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\r\n",
        "                    \r\n",
        "                    else:\r\n",
        "                        logger.info('Dynamically over-write the partition')\r\n",
        "                        spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\r\n",
        "                        df.write.format('delta').mode('overwrite').partitionBy(*partitioning_cols).save(destination_url)\r\n",
        "                else:\r\n",
        "                    logger.info('Dynamically over-write the partition')\r\n",
        "                    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\r\n",
        "                    df.write.format('delta').mode('overwrite').partitionBy(*partitioning_cols).save(destination_url)\r\n",
        "            else:\r\n",
        "                delta_table_sink.alias('sink').merge(df.alias('updates'), f'sink.{primary_key} = updates.{primary_key}').whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\r\n",
        "        else:\r\n",
        "            logger.debug('No existing delta table found. Creating delta table.')\r\n",
        "            if not(partitioning):\r\n",
        "                logger.info('Writing unpartitioned delta lake')\r\n",
        "                df.write.format('delta').save(destination_url)\r\n",
        "            elif partitioning and len(partitioning_cols) == 0:\r\n",
        "                logger.info('Partitioning columns absent - defaulting to DistrictId and SchoolYear as partitioning columns')\r\n",
        "                df.write.format('delta').partitionBy('DistrictId', 'SchoolYear').save(destination_url)\r\n",
        "            else:\r\n",
        "                partitioning_str = ', '.join(partitioning_cols)\r\n",
        "                logger.info(f'Writing partitioned delta lake - partitioned by - {partitioning_str}')\r\n",
        "                df.write.format('delta').partitionBy(*partitioning_cols).save(destination_url)\r\n",
        "\r\n",
        "    def overwrite(self, df, destination_path, primary_key='id', partitioning = False, partitioning_cols = []):\r\n",
        "        \"\"\" Overwrites the existing delta table with the given dataframe.\r\n",
        "            If there is no delta table found in the destination_path, one will be created.    \r\n",
        "        \"\"\"\r\n",
        "        destination_url = self.to_url(destination_path)\r\n",
        "        df = self.fix_column_names(df)\r\n",
        "        \r\n",
        "        if partitioning: \r\n",
        "            df = df.dropDuplicates([primary_key] + partitioning_cols)\r\n",
        "        else:\r\n",
        "            df = df.dropDuplicates([primary_key])\r\n",
        "        if not(partitioning):\r\n",
        "            logger.info('Writing unpartitioned delta lake')\r\n",
        "            df.write.format('delta').mode('overwrite').save(destination_url)\r\n",
        "        elif partitioning and len(partitioning_cols) == 0:\r\n",
        "            logger.info('Partitioning columns absent - defaulting to DistrictId and SchoolYear as partitioning columns')\r\n",
        "            df.write.format('delta').mode('overwrite').partitionBy('DistrictId', 'SchoolYear').save(destination_url)\r\n",
        "        else:\r\n",
        "            partitioning_str = ', '.join(partitioning_cols)\r\n",
        "            logger.info(f'Writing partitioned delta lake - partitioned by - {partitioning_str}')\r\n",
        "            df.write.format('delta').mode('overwrite').partitionBy(*partitioning_cols).save(destination_url)\r\n",
        "        \r\n",
        "    def append(self, df, destination_path, primary_key='id', partitioning = False, partitioning_cols = []):\r\n",
        "        \"\"\" Appends the given dataframe to the delta table in the specified destination.\r\n",
        "            If there is no delta table found in the destination_path, one will be created.    \r\n",
        "        \"\"\"\r\n",
        "        destination_url = self.to_url(destination_path)\r\n",
        "        df = self.fix_column_names(df)\r\n",
        "\r\n",
        "        if partitioning: \r\n",
        "            df = df.dropDuplicates([primary_key] + partitioning_cols)\r\n",
        "        else:\r\n",
        "            df = df.dropDuplicates([primary_key])\r\n",
        "\r\n",
        "        if DeltaTable.isDeltaTable(spark, destination_url):\r\n",
        "            df.write.format('delta').mode('append').save(destination_url)  # https://docs.delta.io/latest/delta-batch.html#append\r\n",
        "        else:\r\n",
        "            logger.debug('No existing delta table found. Creating delta table.')\r\n",
        "            if not(partitioning):\r\n",
        "                logger.info('Writing unpartitioned delta lake')\r\n",
        "                df.write.format('delta').save(destination_url)\r\n",
        "            elif partitioning and len(partitioning_cols) == 0:\r\n",
        "                logger.info('Partitioning columns absent - defaulting to DistrictId and SchoolYear as partitioning columns')\r\n",
        "                df.write.format('delta').partitionBy('DistrictId', 'SchoolYear').save(destination_url)\r\n",
        "            else:\r\n",
        "                partitioning_str = ', '.join(partitioning_cols)\r\n",
        "                logger.info(f'Writing partitioned delta lake - partitioned by - {partitioning_str}')\r\n",
        "                df.write.format('delta').partitionBy(*partitioning_cols).save(destination_url)\r\n",
        "    \r\n",
        "    def get_sink_general_sensitive_paths(self, source_path):\r\n",
        "        path_dict = self.parse_path(source_path)\r\n",
        "        \r\n",
        "        sink_general_path = path_dict['entity_parent_path'].replace('Ingested', 'Refined') + '/general/' + path_dict['entity']\r\n",
        "        sink_sensitive_path = path_dict['entity_parent_path'].replace('Ingested', 'Refined') + '/sensitive/' + path_dict['entity'] + '_lookup'\r\n",
        "\r\n",
        "        return sink_general_path, sink_sensitive_path\r\n",
        "\r\n",
        "    def refine(self, entity_path, metadata=None, primary_key='id'):\r\n",
        "        source_path = f'stage2/Ingested/{entity_path}'\r\n",
        "        primary_key = self.fix_column_name(primary_key) # fix the column name, in case it has a space in it or some other invalid character\r\n",
        "        sink_general_path, sink_sensitive_path = get_sink_general_sensitive_paths(source_path)\r\n",
        "\r\n",
        "        if not metadata:\r\n",
        "            all_metadata = self.get_metadata_from_path(path_dict['entity_parent_path'])\r\n",
        "            metadata = all_metadata[path_dict['entity']]\r\n",
        "        \r\n",
        "        df_changes = self.get_latest_changes(source_path, sink_general_path)\r\n",
        "        spark_schema = self.to_spark_schema(metadata)\r\n",
        "        df_changes = self.modify_schema(df_changes, spark_schema)        \r\n",
        "        if df_changes.count() > 0:\r\n",
        "            df_pseudo, df_lookup = self.pseudonymize(df_changes, metadata)\r\n",
        "            self.upsert(df_pseudo, sink_general_path, f'{primary_key}_pseudonym') # todo: remove this assumption that the primary key will always be hashed during pseduonymization\r\n",
        "            self.upsert(df_lookup, sink_sensitive_path, primary_key)    \r\n",
        "            self.add_to_lake_db(sink_general_path)\r\n",
        "            self.add_to_lake_db(sink_sensitive_path)\r\n",
        "            logger.info(f'Processed {df_changes.count()} updated rows from {source_path} into stage2/Refined')\r\n",
        "        else:\r\n",
        "            logger.info(f'No updated rows in {source_path} to process.')\r\n",
        "        \r\n",
        "        return df_changes.count()\r\n",
        "\r\n",
        "    def pseudonymize(self, df, metadata, transform_mode = False, primary_key = 'id'): #: list[list[str]]):\r\n",
        "        \"\"\" Performs pseudonymization of the given dataframe based on the provided metadata (in the OEA format).\r\n",
        "            For example, if the given df is for an entity called person, \r\n",
        "            2 dataframes will be returned, one called person that has hashed ids and masked fields, \r\n",
        "            and one called person_lookup that contains the original person_id, person_id_pseudo,\r\n",
        "            and the non-masked values for columns marked to be masked.           \r\n",
        "            The lookup table should be written to a \"sensitive\" folder in the data lake.\r\n",
        "            eg, df_pseudo, df_lookup = oea.pseudonymize(df, metadata)\r\n",
        "            [More info on this approach here: https://learn.microsoft.com/en-us/azure/databricks/security/privacy/gdpr-delta#pseudonymize-data]\r\n",
        "        \"\"\"\r\n",
        "        salt = self._get_salt()\r\n",
        "        df_pseudo = df\r\n",
        "        df_lookup = df\r\n",
        "        if not(transform_mode):\r\n",
        "            for row in metadata:\r\n",
        "                col_name = row[0]\r\n",
        "                dtype = row[1]\r\n",
        "                op = row[2]\r\n",
        "                if op == \"hash-no-lookup\" or op == \"hnl\":\r\n",
        "                    # This means that the lookup can be performed against a different table so no lookup is needed.\r\n",
        "                    df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\r\n",
        "                    df_lookup = df_lookup.drop(col_name)           \r\n",
        "                elif op == \"hash\" or op == 'h':\r\n",
        "                    df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\r\n",
        "                    df_lookup = df_lookup.withColumn(col_name + \"_pseudonym\", F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256))\r\n",
        "                elif op == \"mask\" or op == 'm':\r\n",
        "                    df_pseudo = df_pseudo.withColumn(col_name, F.lit('*'))\r\n",
        "                elif op == \"partition-by\":\r\n",
        "                    pass # make no changes for this column so that it will be in both dataframes and can be used for partitioning\r\n",
        "                elif op == \"no-op\" or op == 'x':\r\n",
        "                    df_lookup = df_lookup.drop(col_name)\r\n",
        "        else:\r\n",
        "            col_name = primary_key\r\n",
        "            df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\r\n",
        "            df_lookup = df_lookup.withColumn(col_name + \"_pseudonym\", F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256))\r\n",
        "        \r\n",
        "        return (df_pseudo, df_lookup)\r\n",
        "\r\n",
        "    \r\n",
        "    def add_to_lake_db(self, source_entity_path, overwrite = False, extension = None):\r\n",
        "        \"\"\" Adds the given entity as a table (if the table doesn't already exist) to the proper lake db based on the path.\r\n",
        "            This method will also create the lake db if it doesn't already exist.\r\n",
        "            eg: add_to_lake_db('stage2/Ingested/contoso_sis/v0.1/students')\r\n",
        "\r\n",
        "            Note that a spark db that points to source data in the delta format can't be queried via SQL serverless pool. More info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\r\n",
        "        \"\"\"\r\n",
        "        source_dict = self.parse_path(source_entity_path)\r\n",
        "        \r\n",
        "        db_name = source_dict['ldb_name']\r\n",
        "        if extension is not None:\r\n",
        "            source_dict['entity'] = source_dict['entity'] + str(extension)\r\n",
        "\r\n",
        "        spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\r\n",
        "        if overwrite:\r\n",
        "            spark.sql(f\"drop table if exists {db_name}.{source_dict['entity']}\")\r\n",
        "\r\n",
        "        spark.sql(f\"create table if not exists {db_name}.{source_dict['entity']} using DELTA location '{self.to_url(source_dict['entity_path'])}'\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EdFiClient:\r\n",
        "    #The constructor\r\n",
        "    def __init__(self, workspace, kvName, moduleName, authUrl, dataManagementUrl, changeQueriesUrl, dependenciesUrl, apiVersion, batchLimit, minChangeVer=\"\", maxChangeVer=\"\", schoolYear=None, districtId=None):\r\n",
        "        self.workspace = workspace\r\n",
        "        self.keyvault_linked_service = 'LS_KeyVault'\r\n",
        "        oea.kvName = kvName\r\n",
        "\r\n",
        "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\r\n",
        "        for handler in logging.getLogger().handlers:\r\n",
        "            handler.setFormatter(formatter)           \r\n",
        "        # Customize log level for all loggers\r\n",
        "        logging.getLogger().setLevel(logging.INFO)   \r\n",
        "        logger.info(f\"minChangeVersion={minChangeVer} and maxChangeVersion={maxChangeVer}\")\r\n",
        "\r\n",
        "        if not kvName and workspace == \"dev\":\r\n",
        "            logger.info(\"defaulting to test data\")\r\n",
        "            self.clientId = \"\"\r\n",
        "            self.clientSecret = \"\"\r\n",
        "        else:\r\n",
        "            try:\r\n",
        "                #try to get the credentials from keyvault\r\n",
        "                self.clientId = oea._get_secret(\"edfi-clientid\")\r\n",
        "                self.clientSecret = oea._get_secret(\"edfi-clientsecret\")\r\n",
        "            except Exception as e:\r\n",
        "                #if there was an error getting the credentials\r\n",
        "                #if this is the dev instance proceed with test data, otherwise raise the Exception\r\n",
        "                logger.info(f\"failed to retrieve clientId and clientSecret from keyvault with exception: {str(e)}\")\r\n",
        "                if workspace == \"dev\":\r\n",
        "                    logger.info(\"defaulting to test data\")\r\n",
        "                    self.clientId = \"\"\r\n",
        "                    self.clientSecret = \"\"\r\n",
        "                else:\r\n",
        "                    raise\r\n",
        "        \r\n",
        "        self.authUrl = authUrl\r\n",
        "        self.dataManagementUrl = dataManagementUrl\r\n",
        "        self.changeQueriesUrl = changeQueriesUrl\r\n",
        "        self.dependenciesUrl = dependenciesUrl\r\n",
        "        self.runDate = datetime.utcnow().strftime('%Y-%m-%d')\r\n",
        "        self.authTime = None\r\n",
        "        self.expiresIn = None\r\n",
        "        self.accessToken = None\r\n",
        "        districtPath = districtId if districtId != None else \"All\"\r\n",
        "        schoolYearPath = schoolYear if schoolYear != None else \"All\"\r\n",
        "        self.transactionalFolder = f\"Transactional/{moduleName}/{apiVersion}/DistrictId={districtPath}/SchoolYear={schoolYearPath}\"\r\n",
        "        self.batchLimit = batchLimit\r\n",
        "        self.minChangeVer = minChangeVer\r\n",
        "        self.maxChangeVer = maxChangeVer\r\n",
        "\r\n",
        "    #Method to get the access token for the test data set\r\n",
        "    def authenticateWithAuthorization(self):\r\n",
        "        #TODO: need to update this if we want it to work with other edfi provided test data set versions\r\n",
        "        result = requests.post(\"https://api.ed-fi.org/v5.2/api/oauth/token\",{\"grant_type\":\"client_credentials\"},headers={\"Authorization\":\"Basic UnZjb2hLejl6SEk0OkUxaUVGdXNhTmY4MXh6Q3h3SGZib2xrQw==\"})\r\n",
        "        return result\r\n",
        "\r\n",
        "    #Method to get the access token for a production system with basic auth\r\n",
        "    def authenticateWithBasic(self):\r\n",
        "        authHeader = HTTPBasicAuth(self.clientId, self.clientSecret)\r\n",
        "        result = requests.post(self.authUrl,{\"grant_type\":\"client_credentials\"},auth=authHeader)\r\n",
        "        return result\r\n",
        "\r\n",
        "    #This method orchestrates the authentication\r\n",
        "    def authenticate(self):\r\n",
        "        self.authTime = datetime.now()\r\n",
        "        if not self.clientId or not self.clientSecret: #self.workspace == \"dev\":\r\n",
        "            result = self.authenticateWithAuthorization().json()\r\n",
        "            logger.info(result)\r\n",
        "        else:\r\n",
        "            result = self.authenticateWithBasic().json()\r\n",
        "        self.expiresIn = result[\"expires_in\"]\r\n",
        "        self.accessToken = result[\"access_token\"]\r\n",
        "    \r\n",
        "    #This method manages the access token, refreshing it when required\r\n",
        "    def getAccessToken(self):\r\n",
        "        currentTime = datetime.now()\r\n",
        "        #Get a new access token if none exists, or if the expires time is within 5 minutes of expiry\r\n",
        "        if self.accessToken == None or (currentTime-self.authTime).total_seconds() > self.expiresIn - 300:\r\n",
        "            self.authenticate()\r\n",
        "            return self.accessToken\r\n",
        "        else:\r\n",
        "            return self.accessToken \r\n",
        "\r\n",
        "    def getChangeQueryVersion(self):\r\n",
        "        access_token = self.getAccessToken()\r\n",
        "        response = requests.get(changeQueriesUrl + \"/availableChangeVersions\", headers={\"Authorization\":\"Bearer \" + access_token})\r\n",
        "        return response.json()\r\n",
        "    \r\n",
        "    def getEntities(self):\r\n",
        "        return requests.get(self.dependenciesUrl).json()\r\n",
        "\r\n",
        "    def getDeletes(self,resource, minChangeVersion, maxChangeVersion):\r\n",
        "        url = f\"{self.dataManagementUrl}{resource}/deletes?MinChangeVersion={minChangeVersion}&MaxChangeVersion={maxChangeVersion}\"\r\n",
        "        result = requests.get(url,headers = {\"Authorization\": f\"Bearer {self.getAccessToken()}\"})\r\n",
        "        return result\r\n",
        "\r\n",
        "    def writeToDeletesFile(self, resource, deletes):\r\n",
        "        path = f\"stage1/{self.transactionalFolder}{resource}/delete_batch_data/rundate={self.runDate}/data.json\"\r\n",
        "        mssparkutils.fs.put(oea.to_url(path),deletes.text)\r\n",
        "\r\n",
        "    def landEntities(self, entities = 'All'):\r\n",
        "        if entities == 'All':\r\n",
        "            entities = self.getEntities()\r\n",
        "        else:\r\n",
        "            entities = self.getSpecifiedEntities(entities)\r\n",
        "        changeVersion = self.getChangeQueryVersion()\r\n",
        "        minChangeVersion = changeVersion['OldestChangeVersion'] if self.minChangeVer == None else int(self.minChangeVer)\r\n",
        "        maxChangeVersion = changeVersion['NewestChangeVersion']  if self.maxChangeVer == None else int(self.maxChangeVer)\r\n",
        "        for entity in entities:\r\n",
        "            resource = entity['resource']\r\n",
        "            resourceMinChangeVersion = self.getChangeVersion(resource, minChangeVersion) if self.minChangeVer == None else minChangeVersion\r\n",
        "\r\n",
        "            self.landEntity(resource, resourceMinChangeVersion, maxChangeVersion)\r\n",
        "            deletes = self.getDeletes(resource,resourceMinChangeVersion,maxChangeVersion)\r\n",
        "            if len(deletes.json()):\r\n",
        "                self.writeToDeletesFile(resource,deletes)\r\n",
        "    \r\n",
        "    def getChangeVersion(self, resource, default):\r\n",
        "        path = f\"stage1/{self.transactionalFolder}{resource}/changeFile.json\"\r\n",
        "        if mssparkutils.fs.exists(oea.to_url(path)):\r\n",
        "            return json.loads(mssparkutils.fs.head(oea.to_url(path)))['changeVersion']\r\n",
        "        else:\r\n",
        "            return default\r\n",
        "\r\n",
        "    def landEntity(self,resource,minChangeVersion,maxChangeVersion):\r\n",
        "        logger.info(f\"initiating {resource}\")\r\n",
        "        path = f\"stage1/{self.transactionalFolder}{resource}\"\r\n",
        "        url = f\"{self.dataManagementUrl}{resource}?MinChangeVersion={minChangeVersion}&MaxChangeVersion={maxChangeVersion}&totalCount=true\"\r\n",
        "        total_count_response = requests.get(url, headers={\"Authorization\":f\"Bearer {self.getAccessToken()}\"})\r\n",
        "        try:\r\n",
        "            #Keyset pagination implementation: https://techdocs.ed-fi.org/display/ODSAPIS3V61/Improve+Paging+Performance+on+Large+API+Resources\r\n",
        "            \r\n",
        "            #split into the total number of partitions, and the range size\r\n",
        "            total_count = int(total_count_response.headers[\"Total-Count\"])\r\n",
        "            partitions = total_count // self.batchLimit \r\n",
        "\r\n",
        "            #raise(ValueError('ERROR'))\r\n",
        "            if(total_count == 0 and partitions == 0):\r\n",
        "                logger.info(f'No new / updated items b/w the following versions {minChangeVersion} and {maxChangeVersion}')\r\n",
        "            else:\r\n",
        "                range_size = maxChangeVersion // partitions\r\n",
        "                for i in range(partitions + 1):\r\n",
        "                    #calculate the min and max change version for the partition\r\n",
        "                    partitionMinChangeVersion = i*range_size\r\n",
        "                    partitionMaxChangeVersion = min(maxChangeVersion, (i+1)*range_size)\r\n",
        "\r\n",
        "                    #Calculate the number of batches per partition\r\n",
        "                    partitionUrl=f\"{self.dataManagementUrl}{resource}?MinChangeVersion={partitionChangeVersion}&MaxChangeVersion={partitionChangeVersion}&totalCount=true\"\r\n",
        "                    partition_count_response = requests.get(partitionUrl, headers={\"Authorization\":f\"Bearer {self.getAccessToken()}\"})\r\n",
        "                    partition_count = int(partition_count_response.headers[\"Total-Count\"])\r\n",
        "                    batches = partition_count // self.batchLimit\r\n",
        "\r\n",
        "                    for j in range(batches + 1):\r\n",
        "                        batchUrl=f\"{partitionUrl}&limit={self.batchLimit}&offset={(j)*self.batchLimit}\"\r\n",
        "                        data = requests.get(batch_url, headers={\"Authorization\":f\"Bearer {self.getAccessToken()}\"}) \r\n",
        "                        if(data.status_code < 400):         \r\n",
        "                            filepath = f\"{path}/delta_batch_data/rundate={self.runDate}/data{uuid.uuid4()}.json\"\r\n",
        "                            output = json.loads(data.text)\r\n",
        "                            output_string = \"\"\r\n",
        "                            for line in output:\r\n",
        "                                output_string += json.dumps(line) + \"\\n\"\r\n",
        "                            mssparkutils.fs.put(oea.to_url(filepath),output_string)\r\n",
        "                        else:\r\n",
        "                            logger.info(f\"There was an error retrieving batch data for {resource}\")\r\n",
        "        except:\r\n",
        "            data = requests.get(url, headers={\"Authorization\":f\"Bearer {self.getAccessToken()}\"})          \r\n",
        "            #print(data.text)\r\n",
        "            if(data.status_code < 400):         \r\n",
        "                filepath = f\"{path}/delta_batch_data/rundate={self.runDate}/data{uuid.uuid4()}.json\"\r\n",
        "                output = json.loads(data.text)\r\n",
        "                if(len(output) == 0):\r\n",
        "                    logger.info(f'No new / updated items b/w the following versions {minChangeVersion} and {maxChangeVersion}')\r\n",
        "                else:\r\n",
        "                    output_string = \"\"\r\n",
        "                    for line in output:\r\n",
        "                        output_string += json.dumps(line) + \"\\n\"\r\n",
        "                    mssparkutils.fs.put(oea.to_url(filepath),output_string)\r\n",
        "            else:\r\n",
        "                logger.info(f\"There was an error retrieving data for {resource}\")\r\n",
        "    \r\n",
        "        changeFilepath = f\"{path}/changeFile.json\"\r\n",
        "        changeData = {\"changeVersion\":maxChangeVersion}\r\n",
        "        mssparkutils.fs.put(oea.to_url(changeFilepath),json.dumps(changeData),True)\r\n",
        "        logging.info(f\"completed {resource}\")\r\n",
        "    \r\n",
        "    def parse_text_to_dataframe(self, text_content, delimiter=','):\r\n",
        "        csv_file = StringIO(text_content)\r\n",
        "        df = pd.read_csv(csv_file, delimiter=delimiter) \r\n",
        "        \r\n",
        "        return df\r\n",
        "\r\n",
        "    def extract_entities_for_etl(self, df):\r\n",
        "        concat_list = []\r\n",
        "        entity_names_list = []\r\n",
        "        \r\n",
        "        for index, row in df.iterrows():\r\n",
        "            entity_type = row['entity_type']\r\n",
        "            entity_name = row['entity_name']\r\n",
        "            \r\n",
        "            if entity_type != 'ed-fi':\r\n",
        "                concat_list.append(f'/{entity_type}/{entity_name}')\r\n",
        "            \r\n",
        "            concat_list.append(f'/ed-fi/{entity_name}')\r\n",
        "            entity_names_list.append(entity_name)\r\n",
        "        \r\n",
        "        return concat_list, list(set(entity_names_list))\r\n",
        "\r\n",
        "\r\n",
        "    def getSpecifiedEntities(self, entities_list):\r\n",
        "        data = self.getEntities()\r\n",
        "        entities = [item for item in data if item['resource'] in entities_list]\r\n",
        "        return entities\r\n",
        "\r\n",
        "    def listSpecifiedEntities(self, path): \r\n",
        "        fullpath = path + '/entities-to-extract.csv'\r\n",
        "        pathExists = oea.path_exists(fullpath)\r\n",
        "        if pathExists:\r\n",
        "            csv_str = oea.get_text_from_path(fullpath)\r\n",
        "            csv_pd_df = self.parse_text_to_dataframe(csv_str, delimiter=',')\r\n",
        "            api_entities, entities = self.extract_entities_for_etl(csv_pd_df)\r\n",
        "        else:\r\n",
        "            api_entities = list()\r\n",
        "            entities = list()\r\n",
        "        return api_entities, entities      "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oea = EdFiOEAChild()          \r\n",
        "edfi = EdFiClient(workspace, \r\n",
        "                  kvName, \r\n",
        "                  moduleName, \r\n",
        "                  authUrl, \r\n",
        "                  dataManagementUrl, \r\n",
        "                  changeQueriesUrl, \r\n",
        "                  dependenciesUrl, \r\n",
        "                  apiVersion, \r\n",
        "                  batchLimit, \r\n",
        "                  minChangeVer, \r\n",
        "                  maxChangeVer)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}