{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import uuid\n",
        "from requests.auth import HTTPBasicAuth\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import csv\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "logger = logging.getLogger('EdFiClient')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%run OEA_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "workspace = 'dev'\n",
        "oea.set_workspace(workspace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "class EdFiOEAChild(OEA):\n",
        "    \"\"\" \n",
        "    NOTE: This class inherits features from the base class OEA and therefore,\n",
        "    should be created / executed after running the notebook OEA_py\n",
        "    \"\"\"\n",
        "    def __init__(self, workspace='dev', logging_level=logging.INFO, storage_account=None, keyvault=None, timezone=None):\n",
        "        # Call the base class constructor to initialize inherited attributes\n",
        "        super().__init__(workspace, logging_level, storage_account, keyvault, timezone)\n",
        "    \n",
        "    def pseudonymize(self, df, metadata, transform_mode = False, debugging = True): #: list[list[str]]):\n",
        "        \"\"\" Performs pseudonymization of the given dataframe based on the provided metadata (in the OEA format).\n",
        "            For example, if the given df is for an entity called person, \n",
        "            2 dataframes will be returned, one called person that has hashed ids and masked fields, \n",
        "            and one called person_lookup that contains the original person_id, person_id_pseudo,\n",
        "            and the non-masked values for columns marked to be masked.           \n",
        "            The lookup table should be written to a \"sensitive\" folder in the data lake.\n",
        "            eg, df_pseudo, df_lookup = oea.pseudonymize(df, metadata)\n",
        "            [More info on this approach here: https://learn.microsoft.com/en-us/azure/databricks/security/privacy/gdpr-delta#pseudonymize-data]\n",
        "        \"\"\"\n",
        "        salt = self._get_salt()\n",
        "        df_pseudo = df\n",
        "        df_lookup = df\n",
        "        if transform_mode:\n",
        "            lookup_cols = ['DistrictId', 'SchoolYear']\n",
        "        else:\n",
        "            lookup_cols = []\n",
        "        if debugging:\n",
        "            col_name = 'id'\n",
        "            df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\n",
        "            df_lookup = df_lookup.withColumn(col_name + \"_pseudonym\", F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256))\n",
        "            \n",
        "            lookup_cols.append(col_name)\n",
        "            lookup_cols.append(col_name + \"_pseudonym\")\n",
        "        else:\n",
        "            for row in metadata:\n",
        "                col_name = row[0]\n",
        "                dtype = row[1]\n",
        "                op = row[2]\n",
        "                if op == \"hash-no-lookup\" or op == \"hnl\":\n",
        "                    # This means that the lookup can be performed against a different table so no lookup is needed.\n",
        "                    df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\n",
        "                    df_lookup = df_lookup.drop(col_name)           \n",
        "                elif op == \"hash\" or op == 'h':\n",
        "                    df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\n",
        "                    df_lookup = df_lookup.withColumn(col_name + \"_pseudonym\", F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256))\n",
        "                    \n",
        "                    lookup_cols.append(col_name)\n",
        "                    lookup_cols.append(col_name + \"_pseudonym\")\n",
        "                \n",
        "                elif op == \"mask\" or op == 'm':\n",
        "                    df_pseudo = df_pseudo.withColumn(col_name, F.lit('*'))\n",
        "                elif op == \"partition-by\":\n",
        "                    pass # make no changes for this column so that it will be in both dataframes and can be used for partitioning\n",
        "                elif op == \"no-op\" or op == 'x':\n",
        "                    df_lookup = df_lookup.drop(col_name)\n",
        "\t\t\n",
        "        df_lookup = df_lookup.select(*lookup_cols)\n",
        "        return (df_pseudo, df_lookup)\n",
        "\n",
        "    def upsert(self, df, destination_path, primary_key='id', partitioning=False, partitioning_cols = []):\n",
        "        \"\"\" Upserts the data in the given dataframe into the specified destination using the given primary_key_column to identify the updates.\n",
        "            If there is no delta table found in the destination_path, one will be created.    \n",
        "        \"\"\"\n",
        "        destination_url = self.to_url(destination_path)\n",
        "        df = self.fix_column_names(df)\n",
        "\n",
        "        if partitioning: \n",
        "            df = df.dropDuplicates([primary_key] + partitioning_cols)\n",
        "        else:\n",
        "            df = df.dropDuplicates([primary_key])\n",
        "        if DeltaTable.isDeltaTable(spark, destination_url):\n",
        "            delta_table_sink = DeltaTable.forPath(spark, destination_url)\n",
        "            \n",
        "            if partitioning:\n",
        "                #TODO: Generalize for arbitrary partitioning columns\n",
        "                if (sorted(partitioning_cols) == ['DistrictId', 'SchoolYear']) or (len(partitioning_cols) == 0):\n",
        "                    # Assumption: Each DF should have constant DistrictId and SchoolYear per run\n",
        "                    partitioning_cols = ['DistrictId', 'SchoolYear']\n",
        "                    if (df.select('DistrictId').first() and df.select('DistrictId').first()):\n",
        "                        DistrictId = df.select('DistrictId').first()[0]\n",
        "                        SchoolYear = df.select('SchoolYear').first()[0]\n",
        "                        destination_partition_url = self.to_url(f\"{destination_path}/DistrictId={DistrictId}/SchoolYear={SchoolYear}\")\n",
        "                        if DeltaTable.isDeltaTable(spark, destination_partition_url):\n",
        "                            logger.info('Upsert by Partitions + PK Cols')\n",
        "                            delta_table_sink.alias('sink').merge(df.alias('updates'), f'sink.DistrictId = updates.DistrictId AND sink.SchoolYear = updates.SchoolYear AND sink.{primary_key} = updates.{primary_key}').whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
        "                    \n",
        "                    else:\n",
        "                        logger.info('Dynamically over-write the partition')\n",
        "                        spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
        "                        df.write.format('delta').mode('overwrite').partitionBy(*partitioning_cols).save(destination_url)\n",
        "                else:\n",
        "                    logger.info('Dynamically over-write the partition')\n",
        "                    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
        "                    df.write.format('delta').mode('overwrite').partitionBy(*partitioning_cols).save(destination_url)\n",
        "            else:\n",
        "                delta_table_sink.alias('sink').merge(df.alias('updates'), f'sink.{primary_key} = updates.{primary_key}').whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
        "        else:\n",
        "            logger.debug('No existing delta table found. Creating delta table.')\n",
        "            if not(partitioning):\n",
        "                logger.info('Writing unpartitioned delta lake')\n",
        "                df.write.format('delta').save(destination_url)\n",
        "            elif partitioning and len(partitioning_cols) == 0:\n",
        "                logger.info('Partitioning columns absent - defaulting to DistrictId and SchoolYear as partitioning columns')\n",
        "                df.write.format('delta').partitionBy('DistrictId', 'SchoolYear').save(destination_url)\n",
        "            else:\n",
        "                partitioning_str = ', '.join(partitioning_cols)\n",
        "                logger.info(f'Writing partitioned delta lake - partitioned by - {partitioning_str}')\n",
        "                df.write.format('delta').partitionBy(*partitioning_cols).save(destination_url)\n",
        "\n",
        "    def overwrite(self, df, destination_path, primary_key='id', partitioning = False, partitioning_cols = []):\n",
        "        \"\"\" Overwrites the existing delta table with the given dataframe.\n",
        "            If there is no delta table found in the destination_path, one will be created.    \n",
        "        \"\"\"\n",
        "        destination_url = self.to_url(destination_path)\n",
        "        df = self.fix_column_names(df)\n",
        "        \n",
        "        if partitioning: \n",
        "            df = df.dropDuplicates([primary_key] + partitioning_cols)\n",
        "        else:\n",
        "            df = df.dropDuplicates([primary_key])\n",
        "        if not(partitioning):\n",
        "            logger.info('Writing unpartitioned delta lake')\n",
        "            df.write.format('delta').mode('overwrite').save(destination_url)\n",
        "        elif partitioning and len(partitioning_cols) == 0:\n",
        "            logger.info('Partitioning columns absent - defaulting to DistrictId and SchoolYear as partitioning columns')\n",
        "            df.write.format('delta').mode('overwrite').partitionBy('DistrictId', 'SchoolYear').save(destination_url)\n",
        "        else:\n",
        "            partitioning_str = ', '.join(partitioning_cols)\n",
        "            logger.info(f'Writing partitioned delta lake - partitioned by - {partitioning_str}')\n",
        "            df.write.format('delta').mode('overwrite').partitionBy(*partitioning_cols).save(destination_url)\n",
        "        \n",
        "    def append(self, df, destination_path, primary_key='id', partitioning = False, partitioning_cols = []):\n",
        "        \"\"\" Appends the given dataframe to the delta table in the specified destination.\n",
        "            If there is no delta table found in the destination_path, one will be created.    \n",
        "        \"\"\"\n",
        "        destination_url = self.to_url(destination_path)\n",
        "        df = self.fix_column_names(df)\n",
        "\n",
        "        if partitioning: \n",
        "            df = df.dropDuplicates([primary_key] + partitioning_cols)\n",
        "        else:\n",
        "            df = df.dropDuplicates([primary_key])\n",
        "\n",
        "        if DeltaTable.isDeltaTable(spark, destination_url):\n",
        "            df.write.format('delta').mode('append').save(destination_url)  # https://docs.delta.io/latest/delta-batch.html#append\n",
        "        else:\n",
        "            logger.debug('No existing delta table found. Creating delta table.')\n",
        "            if not(partitioning):\n",
        "                logger.info('Writing unpartitioned delta lake')\n",
        "                df.write.format('delta').save(destination_url)\n",
        "            elif partitioning and len(partitioning_cols) == 0:\n",
        "                logger.info('Partitioning columns absent - defaulting to DistrictId and SchoolYear as partitioning columns')\n",
        "                df.write.format('delta').partitionBy('DistrictId', 'SchoolYear').save(destination_url)\n",
        "            else:\n",
        "                partitioning_str = ', '.join(partitioning_cols)\n",
        "                logger.info(f'Writing partitioned delta lake - partitioned by - {partitioning_str}')\n",
        "                df.write.format('delta').partitionBy(*partitioning_cols).save(destination_url)\n",
        "    \n",
        "    def get_sink_general_sensitive_paths(self, source_path):\n",
        "        path_dict = self.parse_path(source_path)\n",
        "        \n",
        "        sink_general_path = path_dict['entity_parent_path'].replace('Ingested', 'Refined') + '/general/' + path_dict['entity']\n",
        "        sink_sensitive_path = path_dict['entity_parent_path'].replace('Ingested', 'Refined') + '/sensitive/' + path_dict['entity'] + '_lookup'\n",
        "\n",
        "        return sink_general_path, sink_sensitive_path\n",
        "\n",
        "    def refine(self, entity_path, metadata=None, primary_key='id'):\n",
        "        source_path = f'stage2/Ingested/{entity_path}'\n",
        "        primary_key = self.fix_column_name(primary_key) # fix the column name, in case it has a space in it or some other invalid character\n",
        "        sink_general_path, sink_sensitive_path = get_sink_general_sensitive_paths(source_path)\n",
        "\n",
        "        if not metadata:\n",
        "            all_metadata = self.get_metadata_from_path(path_dict['entity_parent_path'])\n",
        "            metadata = all_metadata[path_dict['entity']]\n",
        "        \n",
        "        df_changes = self.get_latest_changes(source_path, sink_general_path)\n",
        "        spark_schema = self.to_spark_schema(metadata)\n",
        "        df_changes = self.modify_schema(df_changes, spark_schema)        \n",
        "        if df_changes.count() > 0:\n",
        "            df_pseudo, df_lookup = self.pseudonymize(df_changes, metadata)\n",
        "            self.upsert(df_pseudo, sink_general_path, f'{primary_key}_pseudonym') # todo: remove this assumption that the primary key will always be hashed during pseduonymization\n",
        "            self.upsert(df_lookup, sink_sensitive_path, primary_key)    \n",
        "            self.add_to_lake_db(sink_general_path)\n",
        "            self.add_to_lake_db(sink_sensitive_path)\n",
        "            logger.info(f'Processed {df_changes.count()} updated rows from {source_path} into stage2/Refined')\n",
        "        else:\n",
        "            logger.info(f'No updated rows in {source_path} to process.')\n",
        "        \n",
        "        return df_changes.count()\n",
        "    \n",
        "    def add_to_lake_db(self, source_entity_path, overwrite = False, extension = None):\n",
        "        \"\"\" Adds the given entity as a table (if the table doesn't already exist) to the proper lake db based on the path.\n",
        "            This method will also create the lake db if it doesn't already exist.\n",
        "            eg: add_to_lake_db('stage2/Ingested/contoso_sis/v0.1/students')\n",
        "\n",
        "            Note that a spark db that points to source data in the delta format can't be queried via SQL serverless pool. More info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\n",
        "        \"\"\"\n",
        "        source_dict = self.parse_path(source_entity_path)\n",
        "        \n",
        "        db_name = source_dict['ldb_name']\n",
        "        if extension is not None:\n",
        "            source_dict['entity'] = source_dict['entity'] + str(extension)\n",
        "\n",
        "        spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\n",
        "        if overwrite:\n",
        "            spark.sql(f\"drop table if exists {db_name}.{source_dict['entity']}\")\n",
        "\n",
        "        spark.sql(f\"create table if not exists {db_name}.{source_dict['entity']} using DELTA location '{self.to_url(source_dict['entity_path'])}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "class EdFiClient:\n",
        "    #The constructor\n",
        "    def __init__(self, workspace, kvName, moduleName, authUrl, dataManagementUrl, changeQueriesUrl, dependenciesUrl, apiVersion, batchLimit, minChangeVer=\"\", maxChangeVer=\"\", schoolYear=None, districtId=None):\n",
        "        self.workspace = workspace\n",
        "        self.keyvault_linked_service = 'LS_KeyVault'\n",
        "        oea.kvName = kvName\n",
        "\n",
        "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "        for handler in logging.getLogger().handlers:\n",
        "            handler.setFormatter(formatter)           \n",
        "        # Customize log level for all loggers\n",
        "        logging.getLogger().setLevel(logging.INFO)   \n",
        "        logger.info(f\"minChangeVersion={minChangeVer} and maxChangeVersion={maxChangeVer}\")\n",
        "\n",
        "        if not kvName and workspace == \"dev\":\n",
        "            logger.info(\"defaulting to test data\")\n",
        "            self.clientId = \"\"\n",
        "            self.clientSecret = \"\"\n",
        "        else:\n",
        "            try:\n",
        "                #try to get the credentials from keyvault\n",
        "                self.clientId = oea._get_secret(\"edfi-clientid\")\n",
        "                self.clientSecret = oea._get_secret(\"edfi-clientsecret\")\n",
        "            except Exception as e:\n",
        "                #if there was an error getting the credentials\n",
        "                #if this is the dev instance proceed with test data, otherwise raise the Exception\n",
        "                logger.info(f\"failed to retrieve clientId and clientSecret from keyvault with exception: {str(e)}\")\n",
        "                if workspace == \"dev\":\n",
        "                    logger.info(\"defaulting to test data\")\n",
        "                    self.clientId = \"\"\n",
        "                    self.clientSecret = \"\"\n",
        "                else:\n",
        "                    raise\n",
        "        \n",
        "        self.authUrl = authUrl\n",
        "        self.dataManagementUrl = dataManagementUrl\n",
        "        self.changeQueriesUrl = changeQueriesUrl\n",
        "        self.dependenciesUrl = dependenciesUrl\n",
        "        self.runDate = datetime.utcnow().strftime('%Y-%m-%d')\n",
        "        self.authTime = None\n",
        "        self.expiresIn = None\n",
        "        self.accessToken = None\n",
        "        districtPath = districtId if districtId != None else \"All\"\n",
        "        schoolYearPath = schoolYear if schoolYear != None else \"All\"\n",
        "        self.transactionalFolder = f\"Transactional/{moduleName}/{apiVersion}/DistrictId={districtPath}/SchoolYear={schoolYearPath}\"\n",
        "        self.batchLimit = batchLimit\n",
        "        self.minChangeVer = minChangeVer\n",
        "        self.maxChangeVer = maxChangeVer\n",
        "\n",
        "    #Method to get the access token for the test data set\n",
        "    def authenticateWithAuthorization(self):\n",
        "        #TODO: need to update this if we want it to work with other edfi provided test data set versions\n",
        "        result = requests.post(\"https://api.ed-fi.org/v5.2/api/oauth/token\",{\"grant_type\":\"client_credentials\"},headers={\"Authorization\":\"Basic UnZjb2hLejl6SEk0OkUxaUVGdXNhTmY4MXh6Q3h3SGZib2xrQw==\"})\n",
        "        return result\n",
        "\n",
        "    #Method to get the access token for a production system with basic auth\n",
        "    def authenticateWithBasic(self):\n",
        "        authHeader = HTTPBasicAuth(self.clientId, self.clientSecret)\n",
        "        result = requests.post(self.authUrl,{\"grant_type\":\"client_credentials\"},auth=authHeader)\n",
        "        return result\n",
        "\n",
        "    #This method orchestrates the authentication\n",
        "    def authenticate(self):\n",
        "        self.authTime = datetime.now()\n",
        "        if not self.clientId or not self.clientSecret: #self.workspace == \"dev\":\n",
        "            result = self.authenticateWithAuthorization().json()\n",
        "            logger.info(result)\n",
        "        else:\n",
        "            result = self.authenticateWithBasic().json()\n",
        "        self.expiresIn = result[\"expires_in\"]\n",
        "        self.accessToken = result[\"access_token\"]\n",
        "    \n",
        "    #This method manages the access token, refreshing it when required\n",
        "    def getAccessToken(self):\n",
        "        currentTime = datetime.now()\n",
        "        #Get a new access token if none exists, or if the expires time is within 5 minutes of expiry\n",
        "        if self.accessToken == None or (currentTime-self.authTime).total_seconds() > self.expiresIn - 300:\n",
        "            self.authenticate()\n",
        "            return self.accessToken\n",
        "        else:\n",
        "            return self.accessToken \n",
        "\n",
        "    def getChangeQueryVersion(self):\n",
        "        access_token = self.getAccessToken()\n",
        "        response = requests.get(changeQueriesUrl + \"/availableChangeVersions\", headers={\"Authorization\":\"Bearer \" + access_token})\n",
        "        return response.json()\n",
        "    \n",
        "    def getEntities(self):\n",
        "        return requests.get(self.dependenciesUrl).json()\n",
        "\n",
        "    def getDeletes(self,resource, minChangeVersion, maxChangeVersion):\n",
        "        url = f\"{self.dataManagementUrl}{resource}/deletes?MinChangeVersion={minChangeVersion}&MaxChangeVersion={maxChangeVersion}\"\n",
        "        result = requests.get(url,headers = {\"Authorization\": f\"Bearer {self.getAccessToken()}\"})\n",
        "        return result\n",
        "\n",
        "    def writeToDeletesFile(self, resource, deletes):\n",
        "        path = f\"stage1/{self.transactionalFolder}{resource}/delete_batch_data/rundate={self.runDate}/data.json\"\n",
        "        mssparkutils.fs.put(oea.to_url(path),deletes.text)\n",
        "\n",
        "    def landEntities(self, entities = 'All'):\n",
        "        if entities == 'All':\n",
        "            entities = self.getEntities()\n",
        "        else:\n",
        "            entities = self.getSpecifiedEntities(entities)\n",
        "        try:\n",
        "            changeVersion = self.getChangeQueryVersion()\n",
        "            minChangeVersion = changeVersion['OldestChangeVersion'] if self.minChangeVer == None else int(self.minChangeVer)\n",
        "            maxChangeVersion = changeVersion['NewestChangeVersion']  if self.maxChangeVer == None else int(self.maxChangeVer)\n",
        "            for entity in entities:\n",
        "                resource = entity['resource']\n",
        "                resourceMinChangeVersion = self.getChangeVersion(resource, minChangeVersion) if self.minChangeVer == None else minChangeVersion\n",
        "\n",
        "                self.landEntity(resource, resourceMinChangeVersion, maxChangeVersion)\n",
        "                deletes = self.getDeletes(resource,resourceMinChangeVersion,maxChangeVersion)\n",
        "                if len(deletes.json()):\n",
        "                    self.writeToDeletesFile(resource,deletes)\n",
        "        except:\n",
        "            for entity in entities:\n",
        "                resource = entity['resource']\n",
        "                # resourceMinChangeVersion = self.getChangeVersion(resource, minChangeVersion) if self.minChangeVer == None else minChangeVersion\n",
        "                \n",
        "                self.landEntity(resource, None, None)\n",
        "                \n",
        "                # deletes = self.getDeletes(resource,resourceMinChangeVersion,maxChangeVersion)\n",
        "                # if len(deletes.json()):\n",
        "                #    self.writeToDeletesFile(resource,deletes)\n",
        "    \n",
        "    def getChangeVersion(self, resource, default):\n",
        "        path = f\"stage1/{self.transactionalFolder}{resource}/changeFile.json\"\n",
        "        if mssparkutils.fs.exists(oea.to_url(path)):\n",
        "            return json.loads(mssparkutils.fs.head(oea.to_url(path)))['changeVersion']\n",
        "        else:\n",
        "            return default\n",
        "\n",
        "    def landEntity(self,resource,minChangeVersion,maxChangeVersion):\n",
        "        logger.info(f\"initiating {resource}\")\n",
        "        path = f\"stage1/{self.transactionalFolder}{resource}\"\n",
        "        try:\n",
        "            url = f\"{self.dataManagementUrl}{resource}?MinChangeVersion={minChangeVersion}&MaxChangeVersion={maxChangeVersion}&totalCount=true\"\n",
        "            total_count_response = requests.get(url, headers={\"Authorization\":f\"Bearer {self.getAccessToken()}\"})\n",
        "            #Keyset pagination implementation: https://techdocs.ed-fi.org/display/ODSAPIS3V61/Improve+Paging+Performance+on+Large+API+Resources\n",
        "            \n",
        "            #split into the total number of partitions, and the range size\n",
        "            total_count = int(total_count_response.headers[\"Total-Count\"])\n",
        "            partitions = total_count // self.batchLimit \n",
        "\n",
        "            #raise(ValueError('ERROR'))\n",
        "            if(total_count == 0 and partitions == 0):\n",
        "                logger.info(f'No new / updated items b/w the following versions {minChangeVersion} and {maxChangeVersion}')\n",
        "            else:\n",
        "                range_size = maxChangeVersion // partitions\n",
        "                for i in range(partitions + 1):\n",
        "                    #calculate the min and max change version for the partition\n",
        "                    partitionMinChangeVersion = i*range_size\n",
        "                    partitionMaxChangeVersion = min(maxChangeVersion, (i+1)*range_size)\n",
        "\n",
        "                    #Calculate the number of batches per partition\n",
        "                    partitionUrl=f\"{self.dataManagementUrl}{resource}?MinChangeVersion={partitionChangeVersion}&MaxChangeVersion={partitionChangeVersion}&totalCount=true\"\n",
        "                    partition_count_response = requests.get(partitionUrl, headers={\"Authorization\":f\"Bearer {self.getAccessToken()}\"})\n",
        "                    partition_count = int(partition_count_response.headers[\"Total-Count\"])\n",
        "                    batches = partition_count // self.batchLimit\n",
        "\n",
        "                    for j in range(batches + 1):\n",
        "                        batchUrl=f\"{partitionUrl}&limit={self.batchLimit}&offset={(j)*self.batchLimit}\"\n",
        "                        data = requests.get(batchUrl, headers={\"Authorization\":f\"Bearer {self.getAccessToken()}\"}) \n",
        "                        if(data.status_code < 400):         \n",
        "                            filepath = f\"{path}/delta_batch_data/rundate={self.runDate}/data{uuid.uuid4()}.json\"\n",
        "                            output = json.loads(data.text)\n",
        "                            output_string = \"\"\n",
        "                            for line in output:\n",
        "                                output_string += json.dumps(line) + \"\\n\"\n",
        "                            mssparkutils.fs.put(oea.to_url(filepath),output_string)\n",
        "                        else:\n",
        "                            logger.info(f\"There was an error retrieving batch data for {resource}\")\n",
        "        except:\n",
        "            url = f\"{self.dataManagementUrl}{resource}?totalCount=true\"\n",
        "            total_count_response = requests.get(url, headers={\"Authorization\":f\"Bearer {self.getAccessToken()}\"})\n",
        "            \n",
        "            data = requests.get(url, headers={\"Authorization\":f\"Bearer {self.getAccessToken()}\"})          \n",
        "            #print(data.text)\n",
        "            if(data.status_code < 400):         \n",
        "                filepath = f\"{path}/delta_batch_data/rundate={self.runDate}/data{uuid.uuid4()}.json\"\n",
        "                try:\n",
        "                    output = json.loads(data.text)\n",
        "                except json.JSONDecodeError as e:\n",
        "                    logger.info(f\"JSON decoding error: {e}\")\n",
        "                \n",
        "                if(len(output) == 0):\n",
        "                    logger.info(f'No new / updated items b/w the following versions {minChangeVersion} and {maxChangeVersion}')\n",
        "                else:\n",
        "                    output_string = \"\"\n",
        "                    for line in output:\n",
        "                        output_string += json.dumps(line) + \"\\n\"\n",
        "                    mssparkutils.fs.put(oea.to_url(filepath),output_string)\n",
        "            else:\n",
        "                logger.info(f\"There was an error retrieving data for {resource}\")\n",
        "    \n",
        "        changeFilepath = f\"{path}/changeFile.json\"\n",
        "        changeData = {\"changeVersion\":maxChangeVersion}\n",
        "        mssparkutils.fs.put(oea.to_url(changeFilepath),json.dumps(changeData),True)\n",
        "        logging.info(f\"completed {resource}\")\n",
        "    \n",
        "    def parse_text_to_dataframe(self, text_content, delimiter=','):\n",
        "        csv_file = StringIO(text_content)\n",
        "        df = pd.read_csv(csv_file, delimiter=delimiter) \n",
        "        \n",
        "        return df\n",
        "\n",
        "    def extract_entities_for_etl(self, df):\n",
        "        concat_list = []\n",
        "        entity_names_list = []\n",
        "        \n",
        "        for index, row in df.iterrows():\n",
        "            entity_type = row['entity_type']\n",
        "            entity_name = row['entity_name']\n",
        "            \n",
        "            if entity_type != 'ed-fi':\n",
        "                concat_list.append(f'/{entity_type}/{entity_name}')\n",
        "            \n",
        "            concat_list.append(f'/ed-fi/{entity_name}')\n",
        "            entity_names_list.append(entity_name)\n",
        "        \n",
        "        return concat_list, list(set(entity_names_list))\n",
        "\n",
        "\n",
        "    def getSpecifiedEntities(self, entities_list):\n",
        "        data = self.getEntities()\n",
        "        entities = [item for item in data if item['resource'] in entities_list]\n",
        "        return entities\n",
        "\n",
        "    def listSpecifiedEntities(self, path): \n",
        "        fullpath = path + '/entities-to-extract.csv'\n",
        "        pathExists = oea.path_exists(fullpath)\n",
        "        if pathExists:\n",
        "            csv_str = oea.get_text_from_path(fullpath)\n",
        "            csv_pd_df = self.parse_text_to_dataframe(csv_str, delimiter=',')\n",
        "            api_entities, entities = self.extract_entities_for_etl(csv_pd_df)\n",
        "        else:\n",
        "            api_entities = list()\n",
        "            entities = list()\n",
        "        return api_entities, entities      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "oea = EdFiOEAChild()          \n",
        "try:\n",
        "    edfi = EdFiClient(workspace, \n",
        "                  kvName, \n",
        "                  moduleName, \n",
        "                  authUrl, \n",
        "                  dataManagementUrl, \n",
        "                  changeQueriesUrl, \n",
        "                  dependenciesUrl, \n",
        "                  apiVersion, \n",
        "                  batchLimit, \n",
        "                  minChangeVer, \n",
        "                  maxChangeVer)\n",
        "except:\n",
        "    pass"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
