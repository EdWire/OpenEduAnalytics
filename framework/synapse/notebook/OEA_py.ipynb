{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "####### OEA configuration #############\n",
        "oea_storage_account = 'yourstorageaccount'\n",
        "oea_keyvault = 'yourkeyvault'\n",
        "oea_timezone = 'US/Eastern'\n",
        "#######################################\n",
        "\n",
        "from delta.tables import DeltaTable\n",
        "from notebookutils import mssparkutils\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType, ShortType, DateType\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.utils import AnalysisException\n",
        "import logging\n",
        "import pandas as pd\n",
        "import sys\n",
        "import re\n",
        "import json\n",
        "import datetime\n",
        "import pytz\n",
        "import random\n",
        "import io\n",
        "import urllib.request\n",
        "\n",
        "logger = logging.getLogger('OEA')\n",
        "\n",
        "class OEA:\n",
        "    \"\"\" OEA (Open Education Analytics) framework simplifies the process of working with large data sets within the context of a lakehouse architecture.\n",
        "        Definition of terms used throughout this codebase:\n",
        "        path - a complete or partial folder or file path (does not include details like scheme or domain name as found in a URL). Ex: contosos/v0.1/students\n",
        "        entity_path - a path that ends with a folder that contains entity data. Ex: contoso/v0.1/students\n",
        "        dataset_path - a path that ends with a folder that contains entity folders (entity parent folder). Ex: contoso/v0.1\n",
        "        url - includes scheme and domain name. Ex: abfss://stage1@storageaccount.dfs.core.windows.net/contoso/v0.1/students\n",
        "\n",
        "    \"\"\"\n",
        "    DELTA_BATCH_DATA = 'delta_batch_data'\n",
        "    ADDITIVE_BATCH_DATA = 'additive_batch_data'\n",
        "    SNAPSHOT_BATCH_DATA = 'snapshot_batch_data'\n",
        "\n",
        "    def __init__(self, workspace='dev', logging_level=logging.INFO, storage_account=None, keyvault=None, timezone=None):\n",
        "        self.keyvault_linked_service = 'LS_KeyVault'\n",
        "        self.salt_secret_name = 'oeaSalt'\n",
        "        self.salt = None\n",
        "        self.workspace = workspace\n",
        "        self.storage_account = oea_storage_account\n",
        "        self.keyvault = oea_keyvault\n",
        "        self.timezone = oea_timezone\n",
        "\n",
        "        # pull in override values if any were passed in\n",
        "        if workspace: self.workspace = workspace\n",
        "        if storage_account: self.storage_account = storage_account\n",
        "        if keyvault: self.keyvault = keyvault \n",
        "        if timezone: self.timezone = timezone\n",
        "        if logging_level: self.logging_level = logging_level    \n",
        "\n",
        "        self._initialize_logger(logging_level)\n",
        "        self.set_workspace(self.workspace)\n",
        "        spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # more info here: https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/optimize-write-for-apache-spark\n",
        "        logger.info(\"OEA initialized.\")\n",
        "\n",
        "    def _initialize_logger(self, logging_level):\n",
        "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "        for handler in logging.getLogger().handlers:\n",
        "            handler.setFormatter(formatter)           \n",
        "        # Customize log level for all loggers\n",
        "        logging.getLogger().setLevel(logging_level)        \n",
        "\n",
        "    def _get_secret(self, secret_name):\n",
        "        \"\"\" Retrieves the specified secret from the keyvault.\n",
        "            This method assumes that the keyvault linked service has been setup and is accessible.\n",
        "        \"\"\"\n",
        "        sc = SparkSession.builder.getOrCreate()\n",
        "        token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
        "        value = token_library.getSecret(self.keyvault, secret_name, self.keyvault_linked_service)        \n",
        "        return value\n",
        "\n",
        "    def _get_salt(self):\n",
        "        if not self.salt:\n",
        "            self.salt = self._get_secret(self.salt_secret_name)\n",
        "        return self.salt\n",
        "\n",
        "    def set_workspace(self, workspace_name):\n",
        "        \"\"\" Allows you to use OEA against your workspace\n",
        "            (eg, you specify Jon as workspace_name, then instead of reading in from stage1 OEA will use workspace/Jon/stage1\n",
        "        \"\"\"\n",
        "        \n",
        "        if workspace_name == 'prod' or workspace_name == 'production':\n",
        "            self.workspace = 'prod'\n",
        "            self.stage1 = 'abfss://stage1@' + self.storage_account + '.dfs.core.windows.net'\n",
        "            self.stage2 = 'abfss://stage2@' + self.storage_account + '.dfs.core.windows.net'\n",
        "            self.stage3 = 'abfss://stage3@' + self.storage_account + '.dfs.core.windows.net'\n",
        "        elif workspace_name == 'dev' or workspace_name == 'development':\n",
        "            self.workspace = 'dev'\n",
        "            self.stage1 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/dev/stage1'\n",
        "            self.stage2 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/dev/stage2'\n",
        "            self.stage3 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/dev/stage3'\n",
        "        else:\n",
        "            self.workspace = workspace_name\n",
        "            self.stage1 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/sandboxes/{workspace_name}/stage1'\n",
        "            self.stage2 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/sandboxes/{workspace_name}/stage2'\n",
        "            self.stage3 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/sandboxes/{workspace_name}/stage3'\n",
        "        logger.info(f'Now using workspace: {self.workspace}')\n",
        "\n",
        "    def to_url(self, path):\n",
        "        \"\"\" Converts the given path into a valid url.\n",
        "            eg, convert_path('stage1/contoso_sis/student') # returns abfss://stage1@storageaccount.dfs.core.windows.net/contoso_sis/student\n",
        "            [Note that the url returned will include the sandbox location if a workspace has been set; for example, abfss://oea@storageaccount.dfs.core.windows.net/sandboxes/sam/stage1/contoso_sis/student]\n",
        "        \"\"\"\n",
        "        if not path or path == '': raise ValueError('Specified path cannot be empty.')\n",
        "        if path.startswith('abfss://'): return path # if a url is given, just return that same url (allows to_url to be invoked just in case translation may be needed)\n",
        "        path_args = path.split('/')\n",
        "        stage = path_args.pop(0)\n",
        "        if stage == 'stage1': stage = self.stage1\n",
        "        elif stage == 'stage2': stage = self.stage2\n",
        "        elif stage == 'stage3': stage = self.stage3\n",
        "        else: raise ValueError(\"Path must begin with either 'stage1', 'stage2', or 'stage3'\")\n",
        "        url = f\"{stage}/{'/'.join(path_args)}\"\n",
        "        logger.debug(f'to_url: {url}')\n",
        "        return url      \n",
        "\n",
        "    def parse_path(self, path):\n",
        "        \"\"\" Parses a path that looks like one of the following:\n",
        "                ms_insights/v0.1\n",
        "                ms_insights/v0.1/students\n",
        "\n",
        "                stage1/Transactional/ms_insights/v0.1\n",
        "                stage1/Transactional/ms_insights/v0.1/students\n",
        "            (the path must either be the path to a specific entity, or the path to the parent folder containing entities)\n",
        "            and returns a dictionary like one of the following:\n",
        "                {'stage': 'stage1', 'stage_num': '1', 'category': 'Transactional', 'source_system': 'contoso_sis', 'entity': None, 'entity_list': ['studentattendance'], 'entity_path': None, 'entity_parent_path': 'stage1/Transactional/contoso_sis/v0.1'}\n",
        "                {'stage': 'stage1', 'stage_num': '1', 'category': 'Transactional', 'source_system': 'contoso_sis', 'entity': 'studentattendance', 'entity_list': None, 'entity_path': 'stage1/Transactional/contoso_sis/v0.1/studentattendance', 'entity_parent_path': 'stage1/Transactional/contoso_sis/v0.1'}\n",
        "\n",
        "            This method assumes the standard OEA data lake, in which paths have this structure: <stage number>/<category>/<source system>/<optional version and partitioning>/<entity>/<either batch_data folder or _delta_log>\n",
        "        \"\"\"\n",
        "        if type(path) is dict: return path # this means the path was already parsed\n",
        "        \n",
        "        ar = path.split('/')\n",
        "        path_dict = {'stage':ar[0], 'stage_num':ar[0][-1], 'category':ar[1], 'source_system':ar[2], 'entity':None, 'entity_list':None, 'entity_path':None, 'entity_parent_path':None}\n",
        "\n",
        "        folders = self.get_folders(self.to_url(path))\n",
        "\n",
        "        # Identify an entity folder by the presence of the \"_delta_log\" folder in stage2 and stage3\n",
        "        if (path_dict['stage_num'] == '1' and ('additive_batch_data' in folders or 'delta_batch_data' in folders or 'snapshot_batch_data' in folders)) or ((path_dict['stage_num'] == '2' or path_dict['stage_num'] == '3') and '_delta_log' in folders):\n",
        "            path_dict['entity'] = ar[-1]\n",
        "            path_dict['entity_path'] = path\n",
        "            path_dict['entity_parent_path'] = '/'.join(ar[0:-1]) # eg, stage1/Transactional/contoso_sis/v0.1\n",
        "        else:\n",
        "            path_dict['entity_list'] = folders\n",
        "            path_dict['entity_parent_path'] = path\n",
        "\n",
        "        if path_dict['stage'] == 'stage2':\n",
        "            abbrev = path_dict['category'][0].lower() # either 'i' for Ingested or 'r' for Refined\n",
        "            path_dict['sdb_name'] = f'sdb_{self.workspace}_s{path_dict[\"stage_num\"]}{abbrev}_{path_dict[\"source_system\"].lower()}' # name of the sql db for this source (use lower case to match the naming for lake db)\n",
        "            path_dict['ldb_name'] = f'ldb_{self.workspace}_s{path_dict[\"stage_num\"]}{abbrev}_{path_dict[\"source_system\"].lower()}' # name of the lake db for this source (spark will automatically lower case the name of the db, but we're doing it here to be explicit)\n",
        "        else:\n",
        "            path_dict['sdb_name'] = f'sdb_{self.workspace}_s{path_dict[\"stage_num\"]}_{path_dict[\"source_system\"].lower()}' # name of the sql db for this source (use lower case to match the naming for lake db)\n",
        "            path_dict['ldb_name'] = f'ldb_{self.workspace}_s{path_dict[\"stage_num\"]}_{path_dict[\"source_system\"].lower()}' # name of the lake db for this source (spark will automatically lower case the name of the db, but we're doing it here to be explicit)\n",
        "        \n",
        "        path_dict['between_path'] = '/'.join(path_dict['entity_parent_path'].split('/')[2:]) # strip off the first 2 args in the entity parent path (eg, strip off stage1/Transactional which leaves contoso_sis/v0.1)\n",
        "\n",
        "        m = re.match(r'.*\\/(v[^\\/]+).*', path_dict['between_path'])\n",
        "        if m:\n",
        "            path_dict['version'] = m.group(1)\n",
        "            # Append the version number to the db names. First replace the '.' char with a 'p' if necessary (because a '.' is not allowed in the db name)\n",
        "            safe_version = re.sub('\\.', 'p', path_dict[\"version\"])\n",
        "            path_dict['sdb_name'] = f'{path_dict[\"sdb_name\"]}_{safe_version}'\n",
        "            path_dict['ldb_name'] = f'{path_dict[\"ldb_name\"]}_{safe_version}'\n",
        "        else:\n",
        "            path_dict['version'] = None\n",
        "\n",
        "        return path_dict\n",
        "    \n",
        "\n",
        "    def rm_if_exists(self, path, recursive_remove=True):\n",
        "        \"\"\" Remove a folder if it exists (defaults to use of recursive removal). \n",
        "            eg. rm_if_exists('stage1/Transactional/contoso_sis/v0.1/students') \n",
        "            will delete the folder 'stage1/Transactional/contoso_sis/v0.1/students'.\n",
        "            Note that the path should always start with stage1/stage2/stage3/oea.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            mssparkutils.fs.rm(self.to_url(path), recursive_remove)\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "    def delete(self, path):\n",
        "        \"\"\" Delete a folder and everything in it. \"\"\"\n",
        "        self.rm_if_exists(path, True)\n",
        "\n",
        "    def ls(self, path):\n",
        "        \"\"\" List the contents of the given path. \"\"\"\n",
        "        url = self.to_url(path)\n",
        "        folders = []\n",
        "        files = []\n",
        "        try:\n",
        "            items = mssparkutils.fs.ls(url)\n",
        "            for item in items:\n",
        "                if item.isFile:\n",
        "                    files.append(item.name)\n",
        "                elif item.isDir:\n",
        "                    folders.append(item.name)\n",
        "        except Exception as e:\n",
        "            logger.warning(\"[OEA] Could not peform ls on specified path: \" + path + \"\\nThis may be because the path does not exist.\")\n",
        "        return (folders, files)\n",
        "\n",
        "    def path_exists(self, path):\n",
        "        \"\"\" Returns true if path exists, false if it doesn't (no exception will be thrown). \n",
        "            eg, path_exists('stage1/mytest/v1.0')\n",
        "        \"\"\"\n",
        "        try:\n",
        "            items = mssparkutils.fs.ls(self.to_url(path))\n",
        "        except Exception as e:\n",
        "            # This Exception comes as a generic Py4JJavaError that occurs when the path specified is not found.\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def get_stage_num(self, path):\n",
        "        \"\"\" Returns the stage number of the given path \"\"\"\n",
        "        m = re.match(r'.*stage(\\d)/.*', path)\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "        else:\n",
        "            raise ValueError(\"Path must begin with either 'stage1', 'stage2', or 'stage3'\")\n",
        "\n",
        "    def get_folders(self, path):\n",
        "        \"\"\" Return the list of folders found in the given path. \"\"\"\n",
        "        dirs = []\n",
        "        try:\n",
        "            items = mssparkutils.fs.ls(self.to_url(path))\n",
        "            for item in items:\n",
        "                #print(item.name, item.isDir, item.isFile, item.path, item.size)\n",
        "                if item.isDir:\n",
        "                    dirs.append(item.name)\n",
        "        except Exception as e:\n",
        "            logger.warning(\"[OEA] Could not get list of folders in specified path: \" + path + \"\\nThis may be because the path does not exist.\")\n",
        "        return dirs\n",
        "\n",
        "    def get_latest_folder(self, path):\n",
        "        \"\"\" Gets the last folder listed in the given path. \"\"\"\n",
        "        folders = self.get_folders(path)\n",
        "        if len(folders) > 0: return folders[-1]\n",
        "        else: return None\n",
        "\n",
        "    def contains_batch_folder(self, path):\n",
        "        \"\"\" Returns True if the given folder contains any OEA batch folders in it, else False\"\"\"\n",
        "        for name in self.get_folders(self.to_url(path)):\n",
        "            if name == 'additive_batch_data' or name == 'snapshot_batch_data' or name == 'delta_batch_data':\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def get_batch_info(self, source_path):\n",
        "        \"\"\" Given a source data path, returns a tuple with the batch type (based on the name of the folder) and file type (based on a file extension) \n",
        "            eg, get_batch_info('stage1/Transactional/sis/v1.0/students') # returns ('snapshot', 'csv')\n",
        "        \"\"\"\n",
        "        url = self.to_url(source_path)\n",
        "        source_folder_name = self.get_latest_folder(url) #expects to find one of: additivie_batch_data, snapshot_batch_data, delta_batch_data\n",
        "        batch_type = source_folder_name.split('_')[0]\n",
        "\n",
        "        rundate_dir = self.get_latest_folder(f'{url}/{source_folder_name}')\n",
        "        data_files = self.ls(f'{url}/{source_folder_name}/{rundate_dir}')[1]\n",
        "        file_extension = data_files[0].split('.')[1]\n",
        "        return batch_type, file_extension        \n",
        "\n",
        "    def load(self, path):\n",
        "        \"\"\" Loads the DELTA table from the given path into a dataframe and returns it \"\"\"\n",
        "        df = spark.read.format('delta').load(self.to_url(path))\n",
        "        return df        \n",
        "\n",
        "    def display(self, path, limit=4):\n",
        "        \"\"\" Displays the DELTA table from the given path in tabular format. \n",
        "            Default limit is 4 \"\"\"\n",
        "        df = spark.read.format('delta').load(self.to_url(path))\n",
        "        display(df.limit(limit))\n",
        "        return df\n",
        "\n",
        "    def show(self, path, limit=4):\n",
        "        \"\"\" Performs a df.show() operation on the dataframe by loading the DELTA table from the given path.\n",
        "            Default limit is 4 \"\"\"\n",
        "        df = spark.read.format('delta').load(self.to_url(path))\n",
        "        df.show(limit)\n",
        "        return df\n",
        "\n",
        "    def fix_column_names(self, df):\n",
        "        \"\"\" Fix column names to satisfy the Parquet naming requirements by substituting invalid characters with an underscore. \"\"\"\n",
        "        df_with_valid_column_names = df.select([F.col(col).alias(self.fix_column_name(col)) for col in df.columns])\n",
        "        return df_with_valid_column_names\n",
        "\n",
        "    def fix_column_name(self, column_name):\n",
        "        \"\"\" Replace illegal characters from a given column name based on DELTA naming conventions by underscores \"\"\"\n",
        "        return re.sub(\"[ ,;{}()\\n\\t=]+\", \"_\", column_name) \n",
        "\n",
        "    def to_spark_schema(self, schema):#: list[list[str]]):\n",
        "        \"\"\" Creates a spark schema from a schema specified in the OEA schema format. \n",
        "            Example:\n",
        "            schemas['Person'] = [['Id','string','hash'],\n",
        "                                    ['CreateDate','timestamp','no-op'],\n",
        "                                    ['LastModifiedDate','timestamp','no-op']]\n",
        "            to_spark_schema(schemas['Person'])\n",
        "        \"\"\"\n",
        "        fields = []\n",
        "        for col_name, dtype, op in schema:\n",
        "            fields.append(StructField(col_name, globals()[dtype.lower().capitalize() + \"Type\"](), True))\n",
        "        spark_schema = StructType(fields)\n",
        "        return spark_schema\n",
        "\n",
        "    def get_text_from_path(self, path):\n",
        "        \"\"\" Returns text contents from a given path \n",
        "            eg: get_text_from_path('stage1/Transactional/contoso_sis/v0.1/students/part1.csv')\n",
        "        \"\"\"\n",
        "        txt = mssparkutils.fs.head(oea.to_url(path), 9000000)\n",
        "        return txt\n",
        "\n",
        "    def get_text_from_url(self, url):\n",
        "        \"\"\" Retrieves the text doc at the given url. \n",
        "            eg: get_text_from_url(\"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/modules/module_catalog/Student_and_School_Data_Systems/metadata.csv\")\n",
        "        \"\"\"\n",
        "        response = urllib.request.urlopen(url)\n",
        "        txt = response.read().decode('utf-8')  \n",
        "        return txt\n",
        "\n",
        "    def get_metadata_from_url(self, url):\n",
        "        \"\"\" Returns the Metadata objects by retrieving the contents of the metadata file from given URL and parsing it.\n",
        "            eg: get_metadata_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/v0.7dev/modules/module_catalog/Student_and_School_Data_Systems/metadata.csv')\n",
        "        \"\"\"\n",
        "        csv_str = self.get_text_from_url(url)\n",
        "        metadata = self.parse_metadata_from_csv(csv_str)\n",
        "        return metadata   \n",
        "\n",
        "    def get_metadata_from_path(self, path):\n",
        "        \"\"\" Returns the Metadata dictionary by retrieving the contents of the metadata file from given path and parsing it.\n",
        "            eg: get_metadata_from_path('stage1/Transactional/contoso_sis/v0.1/metadata.csv')\n",
        "        \"\"\" \n",
        "        csv_str = self.get_text_from_path(path + '/metadata.csv')\n",
        "        metadata = self.parse_metadata_from_csv(csv_str)\n",
        "        return metadata                   \n",
        "\n",
        "    def land_metadata_from_url(self, metadata_url, dataset_path):\n",
        "        \"\"\" Retrieve metadata contents from given URL and write it to file named metadata.csv in the destination path.\n",
        "            eg: land_metadata_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/v0.7dev/modules/module_catalog/Student_and_School_Data_Systems/metadata.csv', 'contoso_sis/v0.1')\n",
        "            Note: Do not include the file name in the dataset_path parameter. \n",
        "        \"\"\"\n",
        "        metadata_str = self.get_text_from_url(metadata_url)\n",
        "        self.write(metadata_str, self._metadata_path(dataset_path))\n",
        "\n",
        "    def _metadata_path(self, dataset_path):\n",
        "        return f'stage2/Ingested/{dataset_path}/metadata.csv'\n",
        "\n",
        "    def parse_metadata_from_csv(self, csv_str):\n",
        "        \"\"\" Parses out metadata from a csv string and returns the metadata dictionary. \n",
        "        \"\"\"\n",
        "        metadata = {}\n",
        "        current_entity = ''\n",
        "        header = None\n",
        "        for line in csv_str.splitlines():\n",
        "            line = line.strip()\n",
        "            # skip empty lines, lines that start with # (because these are comments), and lines with only commas (which is what happens if someone uses excel and leaves a row blank) \n",
        "            if len(line) == 0 or line.startswith('#') or re.match(r'^,+$', line): continue\n",
        "            ar = line.split(',')\n",
        "\n",
        "            if not header:\n",
        "                header = []\n",
        "                for column_name in ar:\n",
        "                    header.append(re.sub(\"[ ,;{}()\\n\\t=]+\", \"_\", column_name))\n",
        "                continue\n",
        "            \n",
        "            # check for the start of a new entity definition\n",
        "            if ar[0] != '':\n",
        "                current_entity = ar[0]\n",
        "                metadata[current_entity] = []\n",
        "            # an attribute row must have an attribute name in the second column\n",
        "            elif len(ar[1]) > 0:\n",
        "                ar = ar[1:] # remove the first element because it will be blank\n",
        "                ar[0] = self.fix_column_name(ar[0]) # remove spaces and other illegal chars from column names\n",
        "                metadata[current_entity].append(ar)\n",
        "            else:\n",
        "                logger.info('Invalid metadata row: ' + line)\n",
        "        return metadata\n",
        "\n",
        "    def write(self, data_str, destination_path_and_filename):\n",
        "        \"\"\" Writes the given data string to a file on blob storage \"\"\"\n",
        "        destination_url = self.to_url(destination_path_and_filename)\n",
        "        mssparkutils.fs.put(destination_url, data_str, True) # Set the last parameter as True to create the file if it does not exist    \n",
        "\n",
        "    def create_run_date(self):\n",
        "        rundate = datetime.datetime.now().replace(microsecond=0) # use UTC for the datetime because when parsing it out later, spark's to_timestamp() assumes the local machine's timezone, and the timezone for the spark cluster will be UTC\n",
        "        return rundate\n",
        "\n",
        "    def land(self, data, entity_path, filename, batch_data_type=DELTA_BATCH_DATA, rundate=None):\n",
        "        \"\"\" Lands data in the given entity_path, adding a rundate folder.\n",
        "            eg, land(data, 'contoso/v0.1/students', 'students.csv', oea.SNAPSHOT_BATCH_DATA)\n",
        "        \"\"\"\n",
        "        if not rundate: rundate = self.create_run_date()\n",
        "        sink_path = f'stage1/Transactional/{entity_path}/{batch_data_type}/rundate={rundate}/{filename}'\n",
        "        self.write(data, sink_path)\n",
        "        return sink_path                  \n",
        "\n",
        "    def upsert(self, df, destination_path, primary_key='id'):\n",
        "        \"\"\" Upserts the data in the given dataframe into the specified destination using the given primary_key_column to identify the updates.\n",
        "            If there is no delta table found in the destination_path, one will be created.    \n",
        "        \"\"\"\n",
        "        destination_url = self.to_url(destination_path)\n",
        "        df = self.fix_column_names(df)\n",
        "        if DeltaTable.isDeltaTable(spark, destination_url):\n",
        "            delta_table_sink = DeltaTable.forPath(spark, destination_url)\n",
        "            #delta_table_sink.alias('sink').option('mergeSchema', 'true').merge(df.alias('updates'), f'sink.{primary_key} = updates.{primary_key}').whenMatchedUpdateAll().whenNotMatchedInsertAll()\n",
        "            delta_table_sink.alias('sink').merge(df.alias('updates'), f'sink.{primary_key} = updates.{primary_key}').whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
        "        else:\n",
        "            logger.debug('No existing delta table found. Creating delta table.')\n",
        "            df.write.format('delta').save(destination_url)\n",
        "\n",
        "    def overwrite(self, df, destination_path):\n",
        "        \"\"\" Overwrites the existing delta table with the given dataframe.\n",
        "            If there is no delta table found in the destination_path, one will be created.    \n",
        "        \"\"\"\n",
        "        destination_url = self.to_url(destination_path)\n",
        "        df = self.fix_column_names(df)\n",
        "        df.write.format('delta').mode('overwrite').save(destination_url)  # https://docs.delta.io/latest/delta-batch.html#overwrite        \n",
        "\n",
        "    def append(self, df, destination_path):\n",
        "        \"\"\" Appends the given dataframe to the delta table in the specified destination.\n",
        "            If there is no delta table found in the destination_path, one will be created.    \n",
        "        \"\"\"\n",
        "        destination_url = self.to_url(destination_path)\n",
        "        df = self.fix_column_names(df)\n",
        "        if DeltaTable.isDeltaTable(spark, destination_url):\n",
        "            df.write.format('delta').mode('append').save(destination_url)  # https://docs.delta.io/latest/delta-batch.html#append\n",
        "        else:\n",
        "            logger.debug('No existing delta table found. Creating delta table.')\n",
        "            df.write.format('delta').save(destination_url)\n",
        "\n",
        "    def process(self, source_path, foreach_batch_function, options={}):\n",
        "        \"\"\" This simplifies the process of using structured streaming when processing transformations.\n",
        "            Provide a source_path and a function that receives a dataframe to work with (which will be a dataframe with data from the given source_path).\n",
        "            Use it like this...\n",
        "            def refine_contoso_dataset(df_source):\n",
        "                metadata = oea.get_metadata_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/v0.7dev/modules/module_catalog/Student_and_School_Data_Systems/metadata.csv')\n",
        "                df_pseudo, df_lookup = oea.pseudonymize(df, metadata['studentattendance'])\n",
        "                oea.upsert(df_pseudo, 'stage2/Refined/contoso_sis/v0.1/studentattendance/general')\n",
        "                oea.upsert(df_lookup, 'stage2/Refined/contoso_sis/v0.1/studentattendance/sensitive')\n",
        "            oea.process('stage2/Ingested/contoso_sis/v0.1/studentattendance', refine_contoso_dataset)             \n",
        "        \"\"\"\n",
        "        if not self.path_exists(source_path):\n",
        "            raise ValueError(f'The given path does not exist: {source_path} (which resolves to: {self.to_url(source_path)})') \n",
        "\n",
        "        def wrapped_function(df, batch_id):\n",
        "            df.persist() # cache the df so it doesn't get read in multiple times when we write to multiple destinations. See: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#foreachbatch\n",
        "            foreach_batch_function(df)\n",
        "            df.unpersist()\n",
        "\n",
        "        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\n",
        "        streaming_df = spark.readStream.format('delta').load(self.to_url(source_path), **options)\n",
        "        # for more info on append vs complete vs update modes for structured streaming: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#basic-concepts\n",
        "        query = streaming_df.writeStream.format('delta').outputMode('append').trigger(once=True).option('checkpointLocation', self.to_url(source_path) + '/_checkpoints').foreachBatch(wrapped_function).start()\n",
        "        query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\n",
        "        number_of_new_inbound_rows = query.lastProgress[\"numInputRows\"]\n",
        "        logger.info(f'Number of new inbound rows processed: {number_of_new_inbound_rows}')\n",
        "        logger.debug(query.lastProgress)\n",
        "        return number_of_new_inbound_rows\n",
        "\n",
        "    def is_entity_path():\n",
        "        # @Gene: I Assume this was for testing purposes, Can we remove it?\n",
        "        return false\n",
        "    \n",
        "    def ingest_all(self, dataset_path, primary_key='id', options={}):\n",
        "        \"\"\" Ingests all the entities in the given source_path.\n",
        "            CSV files are expected to have a header row by default, and JSON files are expected to have complete JSON docs on each row in the file.\n",
        "            To specify options that are different from these defaults, use the options param.\n",
        "            eg, ingest('contoso_sis/v0.1') # ingests all entities found in that path\n",
        "            eg, ingest('contoso_sis/v0.1', options={'header':False}) # for CSV files that don't have a header        \n",
        "        \"\"\"\n",
        "        folders = self.get_folders(self.to_url(f'stage1/Transactional/{dataset_path}'))\n",
        "        number_of_new_inbound_rows = 0\n",
        "        for entity_name in folders:\n",
        "            number_of_new_inbound_rows += self.ingest(f'{dataset_path}/{entity_name}', primary_key, options)\n",
        "        return number_of_new_inbound_rows\n",
        "\n",
        "    def ingest(self, entity_path, primary_key='id', options={}):\n",
        "        \"\"\" Ingests the data for the entity in the given path.\n",
        "            CSV files are expected to have a header row by default, and JSON files are expected to have complete JSON docs on each row in the file.\n",
        "            To specify options that are different from these defaults, use the options param.\n",
        "            eg, ingest('contoso_sis/v0.1/students') # ingests all entities found in that path\n",
        "            eg, ingest('contoso_sis/v0.1/students', options={'header':False}) # for CSV files that don't have a header\n",
        "        \"\"\"\n",
        "        primary_key = self.fix_column_name(primary_key) # fix the column name, in case it has a space in it or some other invalid character\n",
        "        ingested_path = f'stage2/Ingested/{entity_path}'\n",
        "        raw_path = f'stage1/Transactional/{entity_path}'\n",
        "        batch_type, source_data_format = self.get_batch_info(raw_path)\n",
        "        logger.info(f'Ingesting from: {raw_path}, batch type of: {batch_type}, source data format of: {source_data_format}')\n",
        "        source_url = self.to_url(f'{raw_path}/{batch_type}_batch_data')\n",
        "\n",
        "        if batch_type == 'snapshot': source_url = f'{source_url}/{self.get_latest_folder(source_url)}' \n",
        "            \n",
        "        logger.debug(f'Processing {batch_type} data from: {source_url} and writing out to: {ingested_path}')\n",
        "        if batch_type == 'snapshot':\n",
        "            def batch_func(df): self.overwrite(df, ingested_path)\n",
        "        elif batch_type == 'additive':\n",
        "            def batch_func(df): self.append(df, ingested_path)\n",
        "        elif batch_type == 'delta':\n",
        "            def batch_func(df): self.upsert(df, ingested_path, primary_key)\n",
        "        else:\n",
        "            raise ValueError(\"No valid batch folder was found at that path (expected to find a single folder with one of the following names: snapshot_batch_data, additive_batch_data, or delta_batch_data). Are you sure you have the right path?\")                      \n",
        "\n",
        "        if options == None: options = {}\n",
        "        options['format'] = source_data_format # eg, 'csv', 'json'\n",
        "        if source_data_format == 'csv' and (not 'header' in options or options['header'] == None): options['header'] = True  # default to expecting a header in csv files\n",
        "\n",
        "        number_of_new_inbound_rows = self.process(source_url, batch_func, options)\n",
        "        if number_of_new_inbound_rows > 0:    \n",
        "            self.add_to_lake_db(ingested_path)\n",
        "        return number_of_new_inbound_rows\n",
        "\n",
        "    def query(self, source_path, query_str, criteria_str=None):\n",
        "        \"\"\" Returns a dataframe which is the obtained by running the given SQL query on the data in the source_path.\n",
        "        \"\"\"\n",
        "        df = self.load(source_path)\n",
        "        sqlContext.registerDataFrameAsTable(df, 'tmp_source_table')\n",
        "        if criteria_str:\n",
        "            query = f'{query_str} from tmp_source_table where {criteria_str}'\n",
        "        else:\n",
        "            query = f'{query_str} from tmp_source_table'\n",
        "        df = sqlContext.sql(query)\n",
        "        return df       \n",
        "\n",
        "    def get_latest_changes(self, source_path, sink_path):\n",
        "        \"\"\" Returns a dataframe representing the changes in the source data based on the max rundate in the sink data. \n",
        "            If the sink path is not found, all of the data from the source_path is returned (the assumption is that the sink delta table is being created for the first time).\n",
        "            eg, get_latest_changes('stage2/Ingested/contoso/v0.1/students', 'stage2/Refined/contoso/v0.1/students')\n",
        "        \"\"\"   \n",
        "        maxdatetime = None\n",
        "        try:\n",
        "            sink_df = self.query(sink_path, 'select max(rundate) maxdatetime')\n",
        "            maxdatetime = sink_df.first()['maxdatetime']\n",
        "        except AnalysisException as e:\n",
        "            # This means that there is no delta table at the sink_path yet.\n",
        "            # We'll assume that the sink delta table is being created for the first time, meaning that all of the source data should be returned.\n",
        "            pass\n",
        "\n",
        "        changes_df = self.load(source_path)\n",
        "        if maxdatetime:\n",
        "            # filter the source table for the latest changes (using the max rundate in the destination table as the watermark)\n",
        "            changes_df = changes_df.where(f\"rundate > '{maxdatetime}'\")        \n",
        "        return changes_df\n",
        "\n",
        "    def refine_all(self, dataset_path, metadata=None, primary_key='id'):\n",
        "        \"\"\" Refines all the entities in the given source_path.\n",
        "            CSV files are expected to have a header row by default, and JSON files are expected to have complete JSON docs on each row in the file.\n",
        "            To specify options that are different from these defaults, use the options param.\n",
        "            eg, ingest('contoso_sis/v0.1') # ingests all entities found in that path\n",
        "            eg, ingest('contoso_sis/v0.1', options={'header':False}) # for CSV files that don't have a header        \n",
        "        \"\"\"\n",
        "        folders = self.get_folders(self.to_url(f'stage2/Ingested/{dataset_path}'))\n",
        "        number_of_new_inbound_rows = 0\n",
        "        for entity_name in folders:\n",
        "            number_of_new_inbound_rows += self.refine(f'{dataset_path}/{entity_name}', metadata, primary_key)\n",
        "        return number_of_new_inbound_rows\n",
        "\n",
        "    def refine(self, source_path, metadata=None, primary_key='id'):\n",
        "        \"\"\" Refines the data for an entity in the Ingested folder and writes it to the Refined folder.\n",
        "            This method performs the pseudonymization of the ingested folder and writes to General and Sensitive folders under Refined.\n",
        "        \"\"\"\n",
        "        source_path = f'stage2/Ingested/{source_path}'\n",
        "        primary_key = self.fix_column_name(primary_key) # fix the column name, in case it has a space in it or some other invalid character\n",
        "        path_dict = self.parse_path(source_path)\n",
        "        sink_general_path = path_dict['entity_parent_path'].replace('Ingested', 'Refined') + '/general/' + path_dict['entity']\n",
        "        sink_sensitive_path = path_dict['entity_parent_path'].replace('Ingested', 'Refined') + '/sensitive/' + path_dict['entity'] + '_lookup'\n",
        "        if not metadata:\n",
        "            all_metadata = self.get_metadata_from_path(path_dict['entity_parent_path'])\n",
        "            metadata = all_metadata[path_dict['entity']]\n",
        "\n",
        "        df_changes = self.get_latest_changes(source_path, sink_general_path)\n",
        "\n",
        "        if df_changes.count() > 0:\n",
        "            df_pseudo, df_lookup = self.pseudonymize(df_changes, metadata)\n",
        "            self.upsert(df_pseudo, sink_general_path, f'{primary_key}_pseudonym') # todo: remove this assumption that the primary key will always be hashed during pseduonymization\n",
        "            self.upsert(df_lookup, sink_sensitive_path, primary_key)    \n",
        "            self.add_to_lake_db(sink_general_path)\n",
        "            self.add_to_lake_db(sink_sensitive_path)\n",
        "            logger.info(f'Processed {df_changes.count()} updated rows from {source_path} into stage2/Refined')\n",
        "        else:\n",
        "            logger.info(f'No updated rows in {source_path} to process.')\n",
        "        return df_changes.count()\n",
        "\n",
        "    def load_csv(self, source_path, header=True):\n",
        "        \"\"\" Loads a csv file as a dataframe based on the path specified \"\"\"\n",
        "        options = {'format':'csv', 'header':header}\n",
        "        df = spark.read.load(self.to_url(source_path), **options)\n",
        "        return df      \n",
        "\n",
        "    def load_json(self, source_path, multiline=False):\n",
        "        \"\"\" Loads a json file as a dataframe based on the path specified \"\"\"\n",
        "        options = {'format':'json', 'multiline':multiline}\n",
        "        df = spark.read.load(self.to_url(source_path), **options)\n",
        "        return df    \n",
        "\n",
        "    def pseudonymize(self, df, metadata): #: list[list[str]]):\n",
        "        \"\"\" Performs pseudonymization of the given dataframe based on the provided metadata (in the OEA format).\n",
        "            For example, if the given df is for an entity called person, \n",
        "            2 dataframes will be returned, one called person that has hashed ids and masked fields, \n",
        "            and one called person_lookup that contains the original person_id, person_id_pseudo,\n",
        "            and the non-masked values for columns marked to be masked.           \n",
        "            The lookup table should be written to a \"sensitive\" folder in the data lake.\n",
        "            eg, df_pseudo, df_lookup = oea.pseudonymize(df, metadata)\n",
        "            [More info on this approach here: https://learn.microsoft.com/en-us/azure/databricks/security/privacy/gdpr-delta#pseudonymize-data]\n",
        "        \"\"\"\n",
        "        salt = self._get_salt()\n",
        "        df_pseudo = df\n",
        "        df_lookup = df\n",
        "        for row in metadata:\n",
        "            col_name = row[0]\n",
        "            dtype = row[1]\n",
        "            op = row[2]\n",
        "            if op == \"hash-no-lookup\" or op == \"hnl\":\n",
        "                # This means that the lookup can be performed against a different table so no lookup is needed.\n",
        "                df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\n",
        "                df_lookup = df_lookup.drop(col_name)           \n",
        "            elif op == \"hash\" or op == 'h':\n",
        "                df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\n",
        "                df_lookup = df_lookup.withColumn(col_name + \"_pseudonym\", F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256))\n",
        "            elif op == \"mask\" or op == 'm':\n",
        "                df_pseudo = df_pseudo.withColumn(col_name, F.lit('*'))\n",
        "            elif op == \"partition-by\":\n",
        "                pass # make no changes for this column so that it will be in both dataframes and can be used for partitioning\n",
        "            elif op == \"no-op\" or op == 'x':\n",
        "                df_lookup = df_lookup.drop(col_name)\n",
        "        return (df_pseudo, df_lookup)\n",
        "\n",
        "    def add_to_lake_db(self, source_entity_path):\n",
        "        \"\"\" Adds the given entity as a table (if the table doesn't already exist) to the proper lake db based on the path.\n",
        "            This method will also create the lake db if it doesn't already exist.\n",
        "            eg: add_to_lake_db('stage2/Ingested/contoso_sis/v0.1/students')\n",
        "\n",
        "            Note that a spark db that points to source data in the delta format can't be queried via SQL serverless pool. More info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\n",
        "        \"\"\"\n",
        "        source_dict = self.parse_path(source_entity_path)\n",
        "        db_name = source_dict['ldb_name']\n",
        "        spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\n",
        "        spark.sql(f\"create table if not exists {db_name}.{source_dict['entity']} using DELTA location '{self.to_url(source_dict['entity_path'])}'\")\n",
        "\n",
        "    def drop_lake_db(self, db_name):\n",
        "        \"\"\" Deletes the lake db by the given name \"\"\"\n",
        "        spark.sql(f'DROP DATABASE IF EXISTS {db_name} CASCADE')\n",
        "        result = \"Database dropped: \" + db_name\n",
        "        logger.info(result)\n",
        "        return result\n",
        "\n",
        "    def create_sql_db(self, source_path):\n",
        "        \"\"\" Prints out the sql script needed for creating a sql serverless db and set of views. \"\"\"\n",
        "        source_dict = self.parse_path(source_path)\n",
        "        db_name = source_dict['sdb_name']\n",
        "        cmd = '-- Create a new sql script then execute the following in it:\\n'\n",
        "        cmd += f\"IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = '{db_name}')\\nBEGIN\\n  CREATE DATABASE {db_name};\\nEND;\\nGO\\n\"\n",
        "        cmd += f\"USE {db_name};\\nGO\\n\\n\"\n",
        "        cmd += self.create_sql_views(source_dict['entity_parent_path'])\n",
        "        print(cmd)\n",
        "\n",
        "    def create_sql_views(self, source_path):\n",
        "        \"\"\" Returns the SQL script required to create views for the entities under the given path \"\"\"\n",
        "        cmd = ''      \n",
        "        dirs = self.get_folders(source_path)\n",
        "        for table_name in dirs:\n",
        "            cmd += f\"CREATE OR ALTER VIEW {table_name} AS\\n  SELECT * FROM OPENROWSET(BULK '{self.to_url(source_path)}/{table_name}', FORMAT='delta') AS [r];\\nGO\\n\"\n",
        "        return cmd \n",
        "\n",
        "    def drop_sql_db(self, db_name):\n",
        "        \"\"\" Prints the SQL script required to delete the db by the given name \"\"\"\n",
        "        cmd = '-- Create a new sql script then execute the following in it. Alternatively, you can click on the menu next to the SQL db and select \"Delete\"\\n'\n",
        "        cmd += '-- [Note that this does not affect the data in the data lake - this will only delete the sql db that points to that data.]\\n\\n'\n",
        "        cmd += f'DROP DATABASE {db_name}'\n",
        "        print(cmd)       \n",
        "\n",
        "class DataLakeWriter: \n",
        "    \"\"\" Utility class to write data to ADLS.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_destination):\n",
        "        self.root_destination = root_destination\n",
        "\n",
        "    def write(self, path_and_filename, data_str, format='csv'):\n",
        "        mssparkutils.fs.append(f\"{self.root_destination}/{path_and_filename}\", data_str, True) # Set the last parameter as True to create the file if it does not exist\n",
        "\n",
        "oea = OEA()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.7 ('Spark_Dev')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "be620d6548581d4b255e22136663c988a2c6aa20c2fcc1339f2748495f9e98b2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
